"""
Agent æ‰§è¡Œä¸Šä¸‹æ–‡

AgentContext æ˜¯æ¡†æ¶æä¾›ç»™ä¸šåŠ¡ Agent çš„æ¥å£ï¼š
- åªè¯»ä¿¡æ¯ï¼šquery, session_id
- å·¥ä½œæ–¹æ³•ï¼šbuild_messages, invoke_tools, get_output, clarify

è®¾è®¡åŸåˆ™ï¼š
- ä¸šåŠ¡ä¾§åªèƒ½ä½¿ç”¨å…¬å¼€çš„æ–¹æ³•å’Œå±æ€§
- æ¡†æ¶å†…éƒ¨å¯¹è±¡ç§æœ‰åŒ–ï¼Œé˜²æ­¢ä¸šåŠ¡ä¾§è¶Šæƒ
- è®°å¿†ã€LLMã€å·¥å…·ç­‰ç”±æ¡†æ¶è‡ªåŠ¨ç®¡ç†
- å§”æ´¾ç”±æ¡†æ¶å†…éƒ¨å¤„ç†ï¼Œä¸šåŠ¡ä¾§æ— éœ€å…³å¿ƒ
"""

from __future__ import annotations

import logging
from dataclasses import dataclass, field
from typing import TYPE_CHECKING, Any

from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage
from langgraph.prebuilt import ToolNode
from langgraph.types import Command

from src.modules.oneagentic.core.types import Clarification

if TYPE_CHECKING:
    from src.modules.oneagentic.core.agent import AgentSpec
    from src.modules.oneagentic.memory.session_memory import SessionMemory

logger = logging.getLogger(__name__)


class DelegationSignal(Exception):
    """
    å§”æ´¾ä¿¡å·ï¼ˆæ¡†æ¶å†…éƒ¨ï¼‰

    å½“ Agent è°ƒç”¨å§”æ´¾å·¥å…·æ—¶æŠ›å‡ºï¼Œç”± Executor æ•è·å¤„ç†ã€‚
    ä¸šåŠ¡ä¾§ä¸éœ€è¦çŸ¥é“è¿™ä¸ªå¼‚å¸¸çš„å­˜åœ¨ã€‚
    """

    def __init__(self, command: Command):
        self.command = command
        super().__init__(f"Delegation to {command.goto}")


@dataclass
class AgentContext:
    """
    Agent æ‰§è¡Œä¸Šä¸‹æ–‡

    ä¸šåŠ¡ Agent é€šè¿‡æ­¤ä¸Šä¸‹æ–‡ä¸æ¡†æ¶äº¤äº’ã€‚

    å…¬å¼€å±æ€§ï¼ˆåªè¯»ï¼‰ï¼š
    - session_id: ä¼šè¯ ID
    - query: ç”¨æˆ·è¾“å…¥

    å…¬å¼€æ–¹æ³•ï¼š
    - build_messages(system_prompt): æ„å»º LLM æ¶ˆæ¯
    - invoke_tools(messages): æ‰§è¡Œå·¥å…·è°ƒç”¨å¾ªç¯
    - get_output(messages): è·å–ç»“æ„åŒ–è¾“å‡º
    - clarify(message, questions): è¯·æ±‚ç”¨æˆ·æ¾„æ¸…

    ä½¿ç”¨ç¤ºä¾‹ï¼š
    ```python
    async def run(self, ctx: AgentContext) -> AnalysisOutput | Clarification:
        # 1. æ„å»ºæ¶ˆæ¯
        messages = ctx.build_messages(self.SYSTEM_PROMPT)

        # 2. å·¥å…·è°ƒç”¨å¾ªç¯ï¼ˆå§”æ´¾ç”±æ¡†æ¶è‡ªåŠ¨å¤„ç†ï¼‰
        messages = await ctx.invoke_tools(messages)

        # 3. è·å–ç»“æ„åŒ–è¾“å‡º
        output = await ctx.get_output(messages)

        # 4. ä¸šåŠ¡åˆ¤æ–­
        if output.confidence < 0.7:
            return ctx.clarify("éœ€æ±‚ä¸å¤Ÿæ˜ç¡®", output.ambiguities)

        return output
    ```
    """

    # === å…¬å¼€å±æ€§ï¼ˆåªè¯»ï¼‰===
    session_id: str
    """ä¼šè¯ ID"""

    query: str
    """ç”¨æˆ·è¾“å…¥"""

    # === æ¡†æ¶å†…éƒ¨ï¼ˆç§æœ‰åŒ–ï¼‰===
    _spec: AgentSpec = field(default=None, repr=False)
    """Agent è§„æ ¼ï¼ˆæ¡†æ¶å†…éƒ¨ï¼‰"""

    _memory: SessionMemory | None = field(default=None, repr=False)
    """ä¼šè¯è®°å¿†ï¼ˆæ¡†æ¶è‡ªåŠ¨ç®¡ç†ï¼‰"""

    _knowledge_prompt: str = field(default="", repr=False)
    """çŸ¥è¯†ä¸Šä¸‹æ–‡ï¼ˆæ¡†æ¶è‡ªåŠ¨æ³¨å…¥ï¼‰"""

    _llm: Any = field(default=None, repr=False)
    """LLM å®ä¾‹ï¼ˆæ¡†æ¶å†…éƒ¨ï¼‰"""

    _tools: list[Any] = field(default_factory=list, repr=False)
    """å·¥å…·åˆ—è¡¨ï¼ˆæ¡†æ¶å†…éƒ¨ï¼‰"""

    _state: dict = field(default_factory=dict, repr=False)
    """å…±äº«çŠ¶æ€ï¼ˆæ¡†æ¶å†…éƒ¨ï¼‰"""

    _delegation_command: Command | None = field(default=None, repr=False)
    """å§”æ´¾å‘½ä»¤ï¼ˆæ¡†æ¶å†…éƒ¨ï¼‰"""

    _messages: list[BaseMessage] = field(default_factory=list, repr=False)
    """æ¶ˆæ¯å†å²ï¼ˆæ¡†æ¶å†…éƒ¨ï¼‰"""

    # === å…¬å¼€æ–¹æ³• ===

    def build_messages(self, system_prompt: str) -> Any:
        """
        æ„å»º LLM æ¶ˆæ¯

        è‡ªåŠ¨æ³¨å…¥ï¼š
        - ç³»ç»Ÿæç¤ºè¯
        - è®°å¿†ä¸Šä¸‹æ–‡ï¼ˆå¯¹è¯å†å²ï¼‰
        - çŸ¥è¯†ä¸Šä¸‹æ–‡
        - ç”¨æˆ·æŸ¥è¯¢

        å‚æ•°ï¼š
        - system_prompt: Agent çš„ç³»ç»Ÿæç¤ºè¯

        è¿”å›ï¼š
        - æ¶ˆæ¯å¯¹è±¡ï¼ˆä¸šåŠ¡ä¾§ä¸éœ€è¦äº†è§£å…·ä½“ç±»å‹ï¼Œåªéœ€ä¼ é€’ï¼‰
        """
        messages: list[BaseMessage] = [SystemMessage(content=system_prompt)]

        # æ³¨å…¥ä¸Šä¸‹æ–‡
        context_parts = []

        # ä¼šè¯è®°å¿†ï¼ˆæ¡†æ¶è‡ªåŠ¨ç®¡ç†ï¼‰
        if self._memory:
            memory_prompt = self._memory.to_prompt()
            if memory_prompt:
                context_parts.append(memory_prompt)

        # çŸ¥è¯†ä¸Šä¸‹æ–‡ï¼ˆæ¡†æ¶è‡ªåŠ¨æ³¨å…¥ï¼‰
        if self._knowledge_prompt:
            context_parts.append(self._knowledge_prompt)

        if context_parts:
            context_content = "\n\n".join(context_parts)
            messages.append(SystemMessage(content=context_content))

        # ç”¨æˆ·æŸ¥è¯¢
        if self.query:
            messages.append(HumanMessage(content=self.query))

        self._messages = messages
        return messages

    async def invoke_tools(self, messages: Any) -> Any:
        """
        å·¥å…·è°ƒç”¨å¾ªç¯

        æ‰§è¡Œ LLM è°ƒç”¨å’Œå·¥å…·è°ƒç”¨çš„å¾ªç¯ï¼Œç›´åˆ° LLM ä¸å†è°ƒç”¨å·¥å…·ã€‚
        å¦‚æœè°ƒç”¨äº†å§”æ´¾å·¥å…·ï¼Œä¼šæŠ›å‡º DelegationSignal ç”±æ¡†æ¶å¤„ç†ã€‚

        å‚æ•°ï¼š
        - messages: build_messages() è¿”å›çš„æ¶ˆæ¯å¯¹è±¡

        è¿”å›ï¼š
        - æ›´æ–°åçš„æ¶ˆæ¯å¯¹è±¡

        å¼‚å¸¸ï¼š
        - DelegationSignal: å½“è°ƒç”¨å§”æ´¾å·¥å…·æ—¶ï¼ˆæ¡†æ¶å†…éƒ¨å¤„ç†ï¼‰
        """
        if not self._tools:
            # æ²¡æœ‰å·¥å…·ï¼Œç›´æ¥è°ƒç”¨ LLM
            response = await self._llm.ainvoke(messages)
            messages.append(response)
            self._messages = messages
            return messages

        # åˆ›å»º ToolNode
        tool_node = ToolNode(self._tools)
        llm_with_tools = self._llm.bind_tools(self._tools)

        # å‡†å¤‡çŠ¶æ€
        current_state = self._state.copy()

        for iteration in range(1, self._spec.max_iterations + 1):
            # LLM è°ƒç”¨
            response = await llm_with_tools.ainvoke(messages)

            if not response.tool_calls:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç»“æŸ
                messages.append(response)
                break

            messages.append(response)

            # æ—¥å¿—
            for tc in response.tool_calls:
                logger.info(f"ğŸ”§ [{self._spec.name}] è°ƒç”¨å·¥å…·: {tc['name']}")

            # æ‰§è¡Œå·¥å…·
            current_state["messages"] = messages
            result = await tool_node.ainvoke(current_state)

            # æ£€æŸ¥æ˜¯å¦æ˜¯å§”æ´¾å‘½ä»¤
            if isinstance(result, list) and result and isinstance(result[0], Command):
                self._delegation_command = result[0]
                logger.info(f"ğŸ”„ [{self._spec.name}] å§”æ´¾ç»™ {self._delegation_command.goto}")
                self._messages = messages
                # æŠ›å‡ºå§”æ´¾ä¿¡å·ï¼Œç”±æ¡†æ¶å¤„ç†
                raise DelegationSignal(self._delegation_command)

            # æ™®é€šå·¥å…·ç»“æœ
            if isinstance(result, dict):
                new_messages = result.get("messages", [])
            else:
                new_messages = result if isinstance(result, list) else []

            messages.extend(new_messages)

        self._messages = messages
        return messages

    async def get_output(self, messages: Any) -> Any:
        """
        è·å–ç»“æ„åŒ–è¾“å‡º

        æ ¹æ® Agent å£°æ˜çš„ deliverable_schema ç”Ÿæˆç»“æ„åŒ–è¾“å‡ºã€‚
        ä½¿ç”¨é¡¹ç›®ç»Ÿä¸€çš„ parse_structured_output æœºåˆ¶è§£æã€‚

        å‚æ•°ï¼š
        - messages: invoke_tools() è¿”å›çš„æ¶ˆæ¯å¯¹è±¡

        è¿”å›ï¼š
        - deliverable_schema å®ä¾‹
        """
        from src.infrastructure.llm.structured_output import parse_structured_output

        if not self._spec.deliverable_schema:
            # æ²¡æœ‰ schemaï¼Œè¿”å›æœ€åä¸€æ¡æ¶ˆæ¯å†…å®¹
            if messages:
                last_msg = messages[-1]
                if hasattr(last_msg, "content"):
                    return last_msg.content
            return None

        # ä½¿ç”¨ with_structured_outputï¼ˆjson_mode æ–¹æ³•ï¼‰
        llm_structured = self._llm.with_structured_output(
            self._spec.deliverable_schema,
            method="json_mode",
            include_raw=True,
        )
        result = await llm_structured.ainvoke(messages)

        # 1. ç›´æ¥æ˜¯ç›®æ ‡ç±»å‹
        if isinstance(result, self._spec.deliverable_schema):
            return result

        # 2. dict æ ¼å¼ï¼ˆinclude_raw=True çš„è¿”å›ï¼‰
        if isinstance(result, dict):
            # ä¼˜å…ˆä½¿ç”¨å·²è§£æçš„ç»“æœ
            parsed = result.get("parsed")
            if isinstance(parsed, self._spec.deliverable_schema):
                return parsed

            # ä» raw æå–æ–‡æœ¬ï¼Œç”¨ parse_structured_output è§£æ
            raw = result.get("raw")
            if raw:
                content = getattr(raw, "content", None)
                if content:
                    return parse_structured_output(content, self._spec.deliverable_schema)

                # å°è¯•ä» tool_calls æå–
                tool_calls = getattr(raw, "tool_calls", None)
                if tool_calls and isinstance(tool_calls, list) and tool_calls:

                    args = (
                        tool_calls[0].get("args")
                        if isinstance(tool_calls[0], dict)
                        else getattr(tool_calls[0], "args", None)
                    )
                    if isinstance(args, dict):
                        return self._spec.deliverable_schema.model_validate(args)
                    if isinstance(args, str):
                        return parse_structured_output(args, self._spec.deliverable_schema)

        raise ValueError(f"æ— æ³•è·å–ç»“æ„åŒ–è¾“å‡º: {type(result)}")

    def clarify(
        self, message: str, questions: list[str], options: list[dict] | None = None
    ) -> Clarification:
        """
        è¯·æ±‚ç”¨æˆ·æ¾„æ¸…

        å½“ä¸šåŠ¡åˆ¤æ–­éœ€è¦æ›´å¤šä¿¡æ¯æ—¶ä½¿ç”¨ã€‚
        æ¡†æ¶ä¼šæš‚åœæµç¨‹ï¼Œç­‰å¾…ç”¨æˆ·å›å¤ã€‚

        å‚æ•°ï¼š
        - message: æç¤ºä¿¡æ¯
        - questions: éœ€è¦å›ç­”çš„é—®é¢˜åˆ—è¡¨
        - options: å¯é€‰é¡¹ï¼ˆå¯é€‰ï¼‰

        è¿”å›ï¼š
        - Clarification å¯¹è±¡
        """
        return Clarification(
            message=message,
            questions=questions,
            options=options or [],
        )
