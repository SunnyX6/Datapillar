"""
Developer Agentï¼ˆæ•°æ®å¼€å‘ï¼‰

èŒè´£ï¼šä¸ºæ¯ä¸ª Job çš„ Stage ç”Ÿæˆ SQL
- æŒ‰ Job å¤„ç†ï¼Œæ¯ä¸ª Job åŒ…å«å¤šä¸ª Stage
- ä¸ºæ¯ä¸ª Stage ç”Ÿæˆ SQL
- å°†æ‰€æœ‰ Stage çš„ SQL ç»„åˆæˆå®Œæ•´è„šæœ¬
- é€šè¿‡å·¥å…·è·å–è¡¨ç»“æ„ã€åˆ—çº§è¡€ç¼˜ã€å†å² SQLï¼ˆç²¾å‡†åŒ¹é…ï¼‰
"""

import asyncio
import json
import logging
import re

from langchain_core.messages import ToolMessage

from src.infrastructure.llm.client import call_llm
from src.infrastructure.resilience import get_resilience_config
from src.modules.etl.agents.knowledge_agent import AgentType, get_agent_tools
from src.modules.etl.agents.prompt_messages import build_llm_messages
from src.modules.etl.schemas.agent_result import AgentResult
from src.modules.etl.schemas.developer import DeveloperSqlOutput
from src.modules.etl.schemas.review import ReviewResult
from src.modules.etl.schemas.workflow import Job, Stage, Workflow
from src.modules.etl.tools.table import get_lineage_sql, get_table_detail, get_table_lineage

logger = logging.getLogger(__name__)


DEVELOPER_AGENT_SYSTEM_INSTRUCTIONS = """ä½ æ˜¯ Datapillar çš„ DeveloperAgentï¼ˆæ•°æ®å¼€å‘ï¼‰ã€‚

## ä»»åŠ¡
æ ¹æ®"ä»»åŠ¡å‚æ•° JSON"ä¸ºæŒ‡å®š Job ç”Ÿæˆå®Œæ•´ SQL è„šæœ¬ã€‚

## ä»»åŠ¡å‚æ•°ï¼ˆç³»ç»Ÿæ³¨å…¥ï¼‰
ç³»ç»Ÿä¼šæä¾›ï¼š
- user_queryï¼šç”¨æˆ·åŸå§‹éœ€æ±‚
- current_jobï¼šæœ¬æ¬¡éœ€è¦ç”Ÿæˆ SQL çš„ Jobï¼ˆå« stagesï¼‰
- evidenceï¼šå·²è·å–çš„è¯æ®ï¼ˆè¡¨ç»“æ„/åˆ—çº§è¡€ç¼˜/å†å² SQLï¼‰
- review_feedbackï¼šä¸Šä¸€è½® review åé¦ˆï¼ˆå¦‚æœ‰ï¼‰

## å¯ç”¨å·¥å…·

### get_table_detail
æŸ¥è¯¢è¡¨çš„è¯¦ç»†ä¿¡æ¯ï¼ˆå­—æ®µã€ç±»å‹ç­‰ï¼‰ã€‚

### get_table_lineage
æŸ¥è¯¢è¡¨çš„è¡€ç¼˜å…³ç³»ï¼ˆä¸Šä¸‹æ¸¸è¡¨ï¼‰ã€‚

### get_lineage_sql
æ ¹æ®æºè¡¨å’Œç›®æ ‡è¡¨ç²¾å‡†æŸ¥æ‰¾å†å² SQLã€‚

## ç”Ÿæˆè¦æ±‚ï¼ˆä¸¥æ ¼ï¼‰

### 1. åˆ—åˆ«åè§„èŒƒ
æ‰€æœ‰ SELECT å­—æ®µå¿…é¡»ä½¿ç”¨ AS åˆ«åï¼š
- æ™®é€šå­—æ®µï¼š`t.order_id AS order_id`
- è®¡ç®—å­—æ®µï¼š`t.amount * 2 AS double_amount`
- èšåˆå‡½æ•°ï¼š`SUM(t.amount) AS total_amount`

ç›®æ ‡è¡¨åˆ—å¯¹é½ï¼š
- SELECT çš„åˆ«åå¿…é¡»ä¸ç›®æ ‡è¡¨åˆ—åå®Œå…¨ä¸€è‡´

### 2. ä¸´æ—¶è¡¨è§„èŒƒ
- åˆ›å»ºä¸´æ—¶è¡¨å‰å¿…é¡»å…ˆåˆ é™¤ï¼š`DROP TABLE IF EXISTS temp.xxx;`
- ä¸´æ—¶è¡¨æ ¼å¼ï¼š`temp.tmp_<æè¿°æ€§åç§°>`

### 3. å‚è€ƒè¯æ®
- å¿…é¡»å‚è€ƒå†å² SQL ä¸­çš„ JOIN æ¡ä»¶
- å‚è€ƒåˆ—çº§è¡€ç¼˜ä¸­çš„å­—æ®µæ˜ å°„å…³ç³»

## è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰
ç”Ÿæˆå®Œæˆåï¼Œç›´æ¥è¾“å‡ºä»¥ä¸‹ JSON æ ¼å¼ï¼š
```json
{
  "sql": "-- Stage 1: xxx\\nDROP TABLE IF EXISTS temp.tmp_step1;\\nCREATE TABLE temp.tmp_step1 AS\\nSELECT ...\\n\\n-- Stage 2: xxx\\nINSERT OVERWRITE TABLE ..."
}
```

## é‡è¦çº¦æŸ
1. sql å­—æ®µåŒ…å«æ‰€æœ‰ Stage çš„ SQLï¼Œç”¨æ¢è¡Œåˆ†éš”
2. æœ€åä¸€ä¸ª Stage å¿…é¡»å†™å…¥æœ€ç»ˆç›®æ ‡è¡¨
3. ç”Ÿæˆå®Œæˆåç›´æ¥è¾“å‡º JSONï¼Œä¸è¦è°ƒç”¨ä»»ä½•å·¥å…·
"""


class DeveloperAgent:
    """
    æ•°æ®å¼€å‘

    èŒè´£ï¼š
    1. è¯»å–æ¯ä¸ª Job ä¸­çš„ Stage ä¿¡æ¯
    2. é€šè¿‡å·¥å…·è·å–è¡¨ç»“æ„ã€åˆ—çº§è¡€ç¼˜ã€å‚è€ƒ SQL
    3. ä¸ºæ¯ä¸ª Stage ç”Ÿæˆ SQL
    4. å°†æ‰€æœ‰ Stage SQL ç»„åˆä¸ºå®Œæ•´è„šæœ¬
    5. è®°å½•å‚è€ƒçš„ SQL IDï¼Œç”¨äºåç»­æ‰“åˆ†
    """

    def __init__(self):
        self.llm = call_llm(temperature=0.0)
        config = get_resilience_config()
        self.max_retries = config.max_retries
        self.max_iterations = config.max_iterations
        self._referenced_sql_ids: list[str] = []
        self.allowlist = get_agent_tools(AgentType.DEVELOPER)

    async def run(
        self,
        *,
        user_query: str,
        workflow: Workflow,
        review_feedback: ReviewResult | None = None,
        knowledge_agent=None,
    ) -> AgentResult:
        """
        æ‰§è¡Œ SQL ç”Ÿæˆ

        å‚æ•°ï¼š
        - user_query: ç”¨æˆ·è¾“å…¥
        - workflow: å·¥ä½œæµï¼ˆåŒ…å« Jobsï¼‰
        - review_feedback: ä¸Šä¸€è½® review åé¦ˆ
        - knowledge_agent: KnowledgeAgent å®ä¾‹ï¼ˆç”¨äºæŒ‰éœ€æŸ¥è¯¢æŒ‡é’ˆï¼‰

        è¿”å›ï¼š
        - AgentResult: æ‰§è¡Œç»“æœ
        """
        self._referenced_sql_ids = []
        self._knowledge_agent = knowledge_agent

        is_iteration = review_feedback is not None
        if is_iteration:
            logger.info("ğŸ’» DeveloperAgent æ ¹æ® review åé¦ˆé‡æ–°ç”Ÿæˆ SQL")
        else:
            logger.info("ğŸ’» DeveloperAgent å¼€å§‹ç”Ÿæˆ SQL")

        all_errors: list[str] = []
        generated_count = 0
        total_jobs = len(workflow.jobs)

        try:
            # æŒ‰æ‹“æ‰‘åˆ†å±‚å¤„ç†ï¼ŒåŒä¸€å±‚å†…å¹¶è¡Œæ‰§è¡Œ
            layers = workflow.topological_layers()

            for layer_idx, layer in enumerate(layers, 1):
                logger.info(f"ğŸ“¦ å¤„ç†ç¬¬ {layer_idx}/{len(layers)} å±‚: {[j.id for j in layer]}")

                # å¹¶è¡Œå¤„ç†å½“å‰å±‚çš„æ‰€æœ‰ Job
                layer_results = await asyncio.gather(
                    *[
                        self._process_single_job(
                            job=job,
                            user_query=user_query,
                            review_feedback=review_feedback,
                        )
                        for job in layer
                    ]
                )

                # æ£€æŸ¥å½“å‰å±‚æ˜¯å¦å…¨éƒ¨æˆåŠŸ
                layer_has_error = False
                for job, (sql_script, success, errors) in zip(layer, layer_results, strict=True):
                    if success:
                        job.config = {"content": sql_script}
                        job.config_generated = True
                        generated_count += 1
                        logger.info(f"âœ… Job {job.id} SQL ç”ŸæˆæˆåŠŸ ({len(job.stages)} ä¸ª Stage)")
                    else:
                        all_errors.extend(errors)
                        logger.error(f"âŒ Job {job.id} SQL ç”Ÿæˆå¤±è´¥: {errors}")
                        layer_has_error = True

                # å½“å‰å±‚æœ‰å¤±è´¥ï¼Œç»ˆæ­¢åç»­å±‚çš„å¤„ç†
                if layer_has_error:
                    break

            if all_errors or generated_count < total_jobs:
                logger.error(f"âŒ DeveloperAgent å¤±è´¥: {generated_count}/{total_jobs} æˆåŠŸ")
                return AgentResult.failed(
                    summary=f"SQL ç”Ÿæˆå¤±è´¥: {all_errors[0] if all_errors else 'éƒ¨åˆ† Job æœªç”Ÿæˆ'}",
                    error="\n".join(all_errors) if all_errors else "éƒ¨åˆ† Job ç”Ÿæˆå¤±è´¥",
                )

            logger.info(f"âœ… DeveloperAgent å®Œæˆ: å…¨éƒ¨ {generated_count} ä¸ª Job æˆåŠŸ")

            unique_sql_ids = list(set(self._referenced_sql_ids))
            if unique_sql_ids:
                logger.info(f"ğŸ“ å‚è€ƒäº† {len(unique_sql_ids)} ä¸ªå†å² SQL: {unique_sql_ids}")

            # completed æ ‡å‡†ï¼šæ‰€æœ‰ Job å¿…é¡»äº§å‡ºéç©º SQL
            missing_sql_jobs: list[str] = []
            for job in workflow.jobs:
                content = job.config.get("content") if job.config else None
                if not (job.config_generated and isinstance(content, str) and content.strip()):
                    missing_sql_jobs.append(job.id)
            if missing_sql_jobs:
                return AgentResult.failed(
                    summary=f"SQL ç”Ÿæˆä¸å®Œæ•´ï¼Œç¼ºå°‘æœ‰æ•ˆ SQL çš„ Job: {', '.join(missing_sql_jobs)}",
                    error=f"ç¼ºå°‘æœ‰æ•ˆ SQL çš„ Job: {', '.join(missing_sql_jobs)}",
                )

            return AgentResult.completed(
                summary=f"SQL ç”Ÿæˆå®Œæˆ: {generated_count} ä¸ª Job",
                deliverable=workflow,
                # DeveloperAgent çš„äº§ç‰©æ˜¯å¯¹æ¶æ„å¸ˆ plan çš„â€œè¡¥å…¨â€ï¼ˆå¡«å…… SQLï¼‰ï¼Œå¿…é¡»å†™å›åŒä¸€ä»½äº¤ä»˜ç‰©ç±»å‹
                # å¦åˆ™åç»­è¿­ä»£ä¼šè¯»åˆ°æ—§ planï¼Œå¯¼è‡´â€œçœ‹ä¼¼å®Œæˆä½†å®é™…æ²¡æ›´æ–°â€çš„ç”©é”…å¼çŠ¶æ€æ¼‚ç§»ã€‚
                deliverable_type="plan",
            )

        except Exception as e:
            logger.error(f"DeveloperAgent ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            return AgentResult.failed(
                summary=f"SQL ç”Ÿæˆå¤±è´¥: {str(e)}",
                error=str(e),
            )

    def _format_review_feedback(
        self, review_result: ReviewResult | None, previous_sql: str | None = None
    ) -> str:
        """æ ¼å¼åŒ– review åé¦ˆï¼ˆåŒ…å«ä¸Šä¸€è½®é”™è¯¯ SQLï¼‰"""
        if not review_result:
            return ""

        validation_errors = review_result.validation_errors or []
        failed_tests = review_result.failed_tests or 0

        if not validation_errors and failed_tests == 0:
            return ""

        lines = ["\n## âš ï¸ ä¸Šä¸€è½®æµ‹è¯•åé¦ˆï¼ˆå¿…é¡»ä¿®å¤ï¼‰\n"]

        if failed_tests > 0:
            lines.append(f"å¤±è´¥æµ‹è¯•æ•°: {failed_tests}\n")

        if validation_errors:
            lines.append("### é”™è¯¯åˆ—è¡¨")
            for error in validation_errors:
                lines.append(f"- {error}")

        if previous_sql:
            lines.append("\n### ä¸Šä¸€è½®ç”Ÿæˆçš„é”™è¯¯ SQLï¼ˆä¸è¦é‡å¤è¿™äº›é”™è¯¯ï¼ï¼‰")
            lines.append(f"```sql\n{previous_sql[:1500]}\n```")

        lines.append("")
        return "\n".join(lines)

    async def _process_single_job(
        self,
        *,
        job: Job,
        user_query: str,
        review_feedback: ReviewResult | None,
    ) -> tuple[str, bool, list[str]]:
        """
        å¤„ç†å•ä¸ª Job çš„ SQL ç”Ÿæˆï¼ˆå¯å¹¶è¡Œè°ƒç”¨ï¼‰

        è¿”å›ï¼š(sql_script, success, errors)
        """
        if not job.stages:
            return "", False, [f"Job {job.id} æ²¡æœ‰ Stage ä¿¡æ¯"]

        previous_sql = job.config.get("content") if job.config else None
        job_review_feedback = self._format_review_feedback(review_feedback, previous_sql)

        return await self._generate_job_sql(
            user_query=user_query,
            job=job,
            review_feedback=job_review_feedback,
        )

    async def _generate_job_sql(
        self,
        *,
        user_query: str,
        job: Job,
        review_feedback: str = "",
    ) -> tuple[str, bool, list[str]]:
        """ä¸ºæ•´ä¸ª Job ç”Ÿæˆ SQL è„šæœ¬ï¼ˆé€šè¿‡å·¥å…·è·å–çŸ¥è¯†ï¼‰"""
        all_input_tables = set(job.input_tables or [])
        output_table = job.output_table

        table_schemas = await self._fetch_table_schemas(list(all_input_tables))
        column_lineage = await self._fetch_column_lineage(list(all_input_tables), output_table)
        reference_sql = await self._fetch_reference_sql(list(all_input_tables), output_table)

        stages_info = self._format_stages(job.stages)

        for attempt in range(self.max_retries):
            try:
                task_payload = {
                    "user_query": user_query,
                    "current_job": {
                        "id": job.id,
                        "name": job.name,
                        "description": job.description,
                        "type": job.type,
                        "input_tables": job.input_tables,
                        "output_table": job.output_table,
                        "stages_info": stages_info,
                        "stages": [
                            {
                                "stage_id": st.stage_id,
                                "name": st.name,
                                "description": st.description,
                                "input_tables": st.input_tables,
                                "output_table": st.output_table,
                                "is_temp_table": st.is_temp_table,
                            }
                            for st in sorted((job.stages or []), key=lambda s: s.stage_id)
                        ],
                    },
                    "evidence": {
                        "table_schemas": table_schemas,
                        "column_lineage": column_lineage,
                        "reference_sql": reference_sql,
                    },
                    "review_feedback": review_feedback,
                }

                sql = await self._generate_sql(
                    user_query=user_query,
                    task_payload=task_payload,
                )

                if not sql or len(sql) < 20:
                    continue

                if not any(kw in sql.upper() for kw in ["SELECT", "INSERT", "CREATE"]):
                    continue

                return sql, True, []

            except Exception as e:
                logger.error(f"Job {job.id} SQL ç”Ÿæˆå¤±è´¥ (å°è¯• {attempt + 1}): {e}")

        return "", False, [f"Job {job.id} SQL ç”Ÿæˆå¤±è´¥"]

    async def _generate_sql(
        self,
        *,
        user_query: str,
        task_payload: dict,
    ) -> str:
        """
        å¸¦å·¥å…·è°ƒç”¨çš„ SQL ç”Ÿæˆæµç¨‹ï¼š
        1. é¢„å…ˆè°ƒç”¨ KnowledgeAgent è·å–å€™é€‰è¡¨/åˆ—/å€¼åŸŸï¼ˆå¸¦æƒé™è¿‡æ»¤ï¼‰
        2. ç¬¬ä¸€é˜¶æ®µï¼šLLM è°ƒç”¨å·¥å…·è·å–è¡¨ç»“æ„ç­‰ä¿¡æ¯ï¼ˆbind_tools + ToolMessageï¼‰
        3. ç¬¬äºŒé˜¶æ®µï¼šLLM è¾“å‡ºç»“æ„åŒ–ç»“æœï¼ˆwith_structured_output + parse_structured_output å…œåº•ï¼‰
        """
        # é¢„å…ˆæ£€ç´¢çŸ¥è¯†ä¸Šä¸‹æ–‡ï¼ˆå¸¦æƒé™è¿‡æ»¤ï¼‰
        context_payload = None
        if self._knowledge_agent:
            ctx = await self._knowledge_agent.global_search(user_query, top_k=10, min_score=0.5)
            logger.info(f"ğŸ“š çŸ¥è¯†æ£€ç´¢å®Œæˆ: {ctx.summary()}")
            context_payload = ctx.to_llm_context(allowlist=self.allowlist)

        llm_with_tools = self._bind_tools()
        messages = build_llm_messages(
            system_instructions=DEVELOPER_AGENT_SYSTEM_INSTRUCTIONS,
            agent_id="developer_agent",
            user_query=user_query,
            task_payload=task_payload,
            context_payload=context_payload,
        )

        # ç¬¬ä¸€é˜¶æ®µï¼šå·¥å…·è°ƒç”¨æ”¶é›†ä¿¡æ¯
        for _ in range(self.max_iterations):
            response = await llm_with_tools.ainvoke(messages)

            if not response.tool_calls:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿›å…¥ç¬¬äºŒé˜¶æ®µ
                break

            # æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼Œç»“æœæ”¾å…¥ ToolMessage
            messages.append(response)
            for tc in response.tool_calls:
                logger.info(f"ğŸ”§ DeveloperAgent è°ƒç”¨å·¥å…·: {tc['name']}({tc['args']})")

            results = await asyncio.gather(
                *[self._execute_tool(tc["name"], tc["args"]) for tc in response.tool_calls]
            )

            for tc, result in zip(response.tool_calls, results, strict=True):
                messages.append(ToolMessage(content=result, tool_call_id=tc["id"]))

        # ç¬¬äºŒé˜¶æ®µï¼šç»“æ„åŒ–è¾“å‡ºï¼ˆwith_structured_output è®© LLM çŸ¥é“ schemaï¼‰
        output = await self._get_structured_output(messages, DeveloperSqlOutput)
        return self._clean_sql(output.sql)

    async def _get_structured_output(
        self,
        messages: list,
        schema: type[DeveloperSqlOutput],
    ) -> DeveloperSqlOutput:
        """
        è·å–ç»“æ„åŒ–è¾“å‡ºï¼šwith_structured_output(json_mode) + parse_structured_output å…œåº•
        """
        from src.infrastructure.llm.structured_output import parse_structured_output

        # ä½¿ç”¨ json_modeï¼ˆä¸æ˜¯ function_callingï¼Œé¿å…å’Œå·¥å…·è°ƒç”¨æ··æ·†ï¼‰
        llm_structured = self.llm.with_structured_output(
            schema,
            method="json_mode",
            include_raw=True,
        )
        result = await llm_structured.ainvoke(messages)

        # æƒ…å†µ 1ï¼šç›´æ¥è§£ææˆåŠŸ
        if isinstance(result, schema):
            return result

        # æƒ…å†µ 2ï¼šdict æ ¼å¼ï¼ˆinclude_raw=True çš„è¿”å›ï¼‰
        if isinstance(result, dict):
            parsed = result.get("parsed")
            if isinstance(parsed, schema):
                return parsed

            # è§£æå¤±è´¥ï¼Œå°è¯•ä» raw ä¸­æ¢å¤
            parsing_error = result.get("parsing_error")
            raw = result.get("raw")

            if raw:
                raw_text = getattr(raw, "content", None)
                if raw_text:
                    logger.warning(
                        "with_structured_output è§£æå¤±è´¥ï¼Œå°è¯• parse_structured_output å…œåº•"
                    )
                    try:
                        return parse_structured_output(raw_text, schema)
                    except ValueError as e:
                        logger.error(f"parse_structured_output å…œåº•ä¹Ÿå¤±è´¥: {e}")
                        raise

            if parsing_error:
                raise parsing_error

        raise ValueError(f"æ— æ³•è·å–ç»“æ„åŒ–è¾“å‡º: {type(result)}")

    def _bind_tools(self):
        """ç»‘å®šå·¥å…·åˆ° LLM"""
        tool_registry = {
            "get_table_detail": get_table_detail,
            "get_table_lineage": get_table_lineage,
            "get_lineage_sql": get_lineage_sql,
        }
        tools = [tool_registry[name] for name in self.allowlist if name in tool_registry]
        return self.llm.bind_tools(tools)

    @staticmethod
    def _tool_error(message: str, **extra: object) -> str:
        payload: dict[str, object] = {"status": "error", "message": message}
        payload.update(extra)
        return json.dumps(payload, ensure_ascii=False)

    async def _execute_tool(self, tool_name: str, tool_args: dict) -> str:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ˆæ”¯æŒç²¾ç¡®å‚æ•°å’Œæ¨¡ç³Šå‚æ•°ï¼‰"""
        try:
            if tool_name not in self.allowlist:
                return self._tool_error(f"å·¥å…·ä¸åœ¨ allowlist ä¸­: {tool_name}")

            tool_registry = {
                "get_table_detail": get_table_detail,
                "get_table_lineage": get_table_lineage,
                "get_lineage_sql": get_lineage_sql,
            }

            tool_func = tool_registry.get(tool_name)
            if not tool_func:
                return self._tool_error(f"æœªçŸ¥å·¥å…·: {tool_name}")

            # å¤„ç†è¡¨ç›¸å…³å·¥å…·çš„å‚æ•°
            if tool_name in ["get_table_detail", "get_table_lineage"]:
                catalog = tool_args.get("catalog")
                schema = tool_args.get("schema")
                table = tool_args.get("table")

                # å¦‚æœåªæä¾›äº† table_nameï¼Œå°è¯•è§£æè·¯å¾„
                if not (catalog and schema and table):
                    table_name = tool_args.get("table_name") or tool_args.get("table") or ""
                    if not table_name:
                        return self._tool_error("ç¼ºå°‘ table å‚æ•°")

                    # å°è¯•è§£æ schema.table æˆ– catalog.schema.table æ ¼å¼
                    parts = table_name.split(".")
                    if len(parts) >= 3:
                        catalog, schema, table = parts[0], parts[1], parts[2]
                    elif len(parts) == 2:
                        schema, table = parts[0], parts[1]
                        catalog = ""
                    else:
                        # æ— æ³•è§£æï¼Œå°è¯•é€šè¿‡ knowledge_agent æŸ¥æ‰¾
                        if self._knowledge_agent:
                            ctx = await self._knowledge_agent.global_search(
                                table_name, top_k=1, min_score=0.6
                            )
                            if ctx.tables:
                                pointer = ctx.tables[0]
                                catalog = pointer.catalog
                                schema = pointer.schema
                                table = pointer.table
                            else:
                                return self._tool_error(f"æœªæ‰¾åˆ°è¡¨: {table_name}")
                        else:
                            return self._tool_error(f"æ— æ³•è§£æè¡¨å: {table_name}")

                # æ„é€ ç²¾ç¡®å‚æ•°
                precise_args = {
                    "catalog": catalog,
                    "schema": schema,
                    "table": table,
                }
                if tool_name == "get_table_lineage":
                    precise_args["direction"] = tool_args.get("direction", "both")

                logger.info(f"ğŸ”§ è°ƒç”¨å·¥å…·: {tool_name}({precise_args})")
                return await tool_func.ainvoke(precise_args)

            # get_lineage_sql ç›´æ¥ä½¿ç”¨å‚æ•°
            logger.info(f"ğŸ”§ è°ƒç”¨å·¥å…·: {tool_name}({tool_args})")
            return await tool_func.ainvoke(tool_args or {})

        except Exception as e:
            logger.error(f"å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥: {e}")
            return self._tool_error(str(e))

    def _format_stages(self, stages: list[Stage]) -> str:
        """æ ¼å¼åŒ– Stage åˆ—è¡¨"""
        lines = []
        for stage in sorted(stages, key=lambda s: s.stage_id):
            lines.append(f"### Stage {stage.stage_id}: {stage.name}")
            lines.append(f"- æè¿°: {stage.description}")
            lines.append(f"- è¾“å…¥è¡¨: {', '.join(stage.input_tables)}")
            lines.append(f"- è¾“å‡ºè¡¨: {stage.output_table}")
            lines.append(f"- æ˜¯å¦ä¸´æ—¶è¡¨: {stage.is_temp_table}")
            lines.append("")
        return "\n".join(lines)

    async def _fetch_table_schemas(self, input_tables: list[str]) -> str:
        """é€šè¿‡å·¥å…·è·å–è¡¨ç»“æ„ä¿¡æ¯"""
        lines = []
        for table_name in input_tables:
            try:
                result = await self._execute_tool(
                    "get_table_detail",
                    {"table_name": table_name},
                )
                data = json.loads(result)
                if data.get("status") == "success":
                    columns = data.get("columns", [])
                    col_info = [
                        f"{c.get('name')} ({c.get('data_type', 'string')})" for c in columns[:20]
                    ]
                    lines.append(f"### {table_name}")
                    lines.append(f"å­—æ®µ: {', '.join(col_info)}")
                    if len(columns) > 20:
                        lines.append(f"...å…± {len(columns)} ä¸ªå­—æ®µ")
                    lines.append("")
            except Exception as e:
                logger.warning(f"è·å–è¡¨ {table_name} ç»“æ„å¤±è´¥: {e}")
        return "\n".join(lines) if lines else "ï¼ˆæ— ï¼‰"

    async def _fetch_column_lineage(
        self,
        input_tables: list[str],
        output_table: str | None,
    ) -> str:
        """é€šè¿‡ get_table_lineage è·å–åˆ—çº§è¡€ç¼˜ä¿¡æ¯"""
        if not output_table or not input_tables:
            return "ï¼ˆæ— ï¼‰"
        lines = []
        try:
            # è°ƒç”¨ get_table_lineage è·å–ç›®æ ‡è¡¨çš„ä¸Šæ¸¸è¡€ç¼˜ï¼ˆå«åˆ—æ˜ å°„ï¼‰
            result = await self._execute_tool(
                "get_table_lineage",
                {"table_name": output_table, "direction": "upstream"},
            )
            data = json.loads(result)
            if data.get("status") == "success":
                lineage_edges = data.get("lineage_edges", [])
                input_set = set(input_tables)
                for edge in lineage_edges:
                    source = edge.get("source_table", "")
                    # åŒ¹é…è¾“å…¥è¡¨
                    if source in input_set or any(source.endswith(f".{t}") for t in input_set):
                        mappings = edge.get("column_mappings", [])
                        if mappings:
                            lines.append(f"### {source} â†’ {output_table}")
                            for m in mappings[:20]:
                                src_col = m.get("source_column", "")
                                tgt_col = m.get("target_column", "")
                                transform = m.get("transformation", "direct")
                                if src_col and tgt_col:
                                    lines.append(f"- {src_col} â†’ {tgt_col} ({transform})")
                            lines.append("")
        except Exception as e:
            logger.warning(f"è·å–åˆ—çº§è¡€ç¼˜å¤±è´¥: {e}")
        return "\n".join(lines) if lines else "ï¼ˆæ— ï¼‰"

    async def _fetch_reference_sql(
        self,
        input_tables: list[str],
        output_table: str | None,
    ) -> str:
        """é€šè¿‡å·¥å…·ç²¾å‡†åŒ¹é…å†å² SQLï¼ˆæ ¹æ®è¡€ç¼˜å…³ç³»ï¼‰"""
        if not input_tables or not output_table:
            return "ï¼ˆæ— å†å² SQLï¼‰"
        try:
            result = await self._execute_tool(
                "get_lineage_sql",
                {"source_tables": input_tables, "target_table": output_table},
            )
            data = json.loads(result)
            if data.get("status") == "success":
                sql_content = data.get("sql_content")
                sql_id = data.get("sql_id")
                if sql_id:
                    self._referenced_sql_ids.append(sql_id)
                    logger.info(f"ğŸ“Œ å‚è€ƒå†å² SQL: {sql_id}")
                if sql_content:
                    sql_name = data.get("sql_name", "")
                    engine = data.get("engine", "")
                    return f"""### å†å² SQLï¼ˆ{sql_name}ï¼Œå¼•æ“: {engine}ï¼‰
ç›´æ¥å‚è€ƒæ­¤ SQL çš„ JOIN æ¡ä»¶å’Œå†™æ³•é£æ ¼ï¼

```sql
{sql_content}
```
"""
        except Exception as e:
            logger.warning(f"è·å–å†å² SQL å¤±è´¥: {e}")
        return "ï¼ˆæ— å†å² SQLï¼‰"

    @staticmethod
    def _clean_sql(content: str) -> str:
        """æ¸…ç† SQLï¼ˆå»æ‰ markdown ä»£ç å—ï¼‰"""
        sql_match = re.search(r"```sql\s*([\s\S]*?)\s*```", content)
        if sql_match:
            return sql_match.group(1).strip()
        code_match = re.search(r"```\s*([\s\S]*?)\s*```", content)
        if code_match:
            return code_match.group(1).strip()
        return content.strip()
