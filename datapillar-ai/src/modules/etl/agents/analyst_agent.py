"""
Analyst Agentï¼ˆéœ€æ±‚åˆ†æå¸ˆï¼‰

èŒè´£ï¼šä¸šåŠ¡å±‚é¢çš„éœ€æ±‚åˆ†æä¸æ”¶æ•›
- å°†ç”¨æˆ·éœ€æ±‚æ‹†åˆ†ä¸ºä¸šåŠ¡æ­¥éª¤ï¼ˆStepï¼‰
- åŸºäºçŸ¥è¯†åº“éªŒè¯éœ€æ±‚çš„å¯è¡Œæ€§
- éœ€æ±‚å¿…é¡»åœ¨æ­¤é˜¶æ®µæ”¶æ•›æ¸…æ¥šï¼Œä¸å…è®¸æ¨¡ç³Šéœ€æ±‚å¾€åä¼ 
- é€šè¿‡å·¥å…·éªŒè¯è¡¨æ˜¯å¦å­˜åœ¨
"""

import json
import logging
import re

from langchain_core.messages import AIMessage, HumanMessage, ToolMessage
from langgraph.types import Command

from src.infrastructure.llm.client import call_llm
from src.modules.etl.schemas.kg_context import AgentScopedContext, AgentType, GlobalKGContext
from src.modules.etl.schemas.requirement import AnalysisResult, Ambiguity, DataTarget, Step
from src.modules.etl.schemas.state import AgentState
from src.modules.etl.tools.agent_tools import search_assets, get_table_columns

logger = logging.getLogger(__name__)


ANALYST_AGENT_PROMPT = """ä½ æ˜¯èµ„æ·±æ•°æ®éœ€æ±‚åˆ†æå¸ˆã€‚

## ä»»åŠ¡
æ ¹æ®çŸ¥è¯†ä¸Šä¸‹æ–‡ï¼Œå°†ç”¨æˆ·éœ€æ±‚æ”¶æ•›ä¸ºæ˜ç¡®çš„ä¸šåŠ¡æ­¥éª¤ã€‚

## æ ¸å¿ƒåŸåˆ™
1. **éœ€æ±‚å¿…é¡»æ”¶æ•›** - æ¯ä¸ª Step å¿…é¡»æœ‰æ˜ç¡®çš„è¾“å…¥è¡¨å’Œè¾“å‡ºè¡¨
2. **åŸºäºçŸ¥è¯†åº“** - æºå¤´è¾“å…¥è¡¨å’Œæœ€ç»ˆç›®æ ‡è¡¨å¿…é¡»åœ¨çŸ¥è¯†åº“ä¸­å­˜åœ¨
3. **æ‹’ç»æ¨¡ç³Šéœ€æ±‚** - å¦‚æœéœ€æ±‚æ— æ³•æ”¶æ•›ï¼Œå¿…é¡»è¿”å›å…·ä½“é—®é¢˜è®©ç”¨æˆ·æ¾„æ¸…

## çŸ¥è¯†ä¸Šä¸‹æ–‡

### å¯ç”¨çš„è¡¨ï¼ˆKnowledgeAgent å‘ç°çš„ç›¸å…³è¡¨ï¼‰
{discovered_tables}

### å…¨å±€è¡¨æ¸…å•ï¼ˆå¯¼èˆªï¼‰
{tables_summary}

### è¡¨çº§è¡€ç¼˜ï¼ˆå·²æœ‰çš„æ•°æ®æµå‘ï¼‰
{lineage_summary}

### å¯ç”¨å·¥å…·
{tools_description}

## ç”¨æˆ·éœ€æ±‚
{user_query}

## åˆ†æè¦æ±‚

0. **çŸ¥è¯†åº“ä¸ºç©ºæ£€æŸ¥**ï¼š
   - å¦‚æœ"å¯ç”¨çš„è¡¨"å’Œ"å…¨å±€è¡¨æ¸…å•"éƒ½ä¸ºç©ºï¼Œè¯´æ˜çŸ¥è¯†åº“å°šæœªåˆå§‹åŒ–
   - æ­¤æ—¶å¿…é¡»ç›´æ¥è¿”å›æç¤ºï¼š
     - summary: "å½“å‰çŸ¥è¯†åº“ä¸ºç©ºï¼Œæ— æ³•å¤„ç†æ‚¨çš„éœ€æ±‚"
     - steps: []
     - ambiguities: åŒ…å«ä¸€æ¡å»ºè®®ï¼Œè¯´æ˜éœ€è¦å…ˆåœ¨ Gravitino ä¸­åˆ›å»ºæ•°æ®è¡¨å¹¶åŒæ­¥å…ƒæ•°æ®
     - confidence: 0

1. **éªŒè¯å¯è¡Œæ€§**ï¼š
   - ç”¨æˆ·æåˆ°çš„**æºå¤´è¾“å…¥è¡¨**å¿…é¡»åœ¨çŸ¥è¯†åº“ä¸­å­˜åœ¨
   - ç”¨æˆ·æåˆ°çš„**æœ€ç»ˆç›®æ ‡è¡¨**å¿…é¡»åœ¨çŸ¥è¯†åº“ä¸­å­˜åœ¨
   - å¦‚æœä¸ç¡®å®šè¡¨æ˜¯å¦å­˜åœ¨ï¼Œä½¿ç”¨ search_assets æˆ– get_table_columns å·¥å…·éªŒè¯

2. **æ˜ç¡®è¾“å…¥è¾“å‡º**ï¼š
   - æ¯ä¸ª Step å¿…é¡»æœ‰ input_tablesï¼ˆä»å“ªè¯»ï¼‰
   - æ¯ä¸ª Step å¿…é¡»æœ‰ output_tableï¼ˆå†™åˆ°å“ªï¼‰

3. **éœ€æ±‚æ”¶æ•›**ï¼š
   - å¦‚æœéœ€æ±‚æ¨¡ç³Šï¼ˆå¦‚"å¤„ç†ç”¨æˆ·æ•°æ®"ï¼‰ï¼Œå¿…é¡»è¦æ±‚ç”¨æˆ·æ˜ç¡®
   - å¦‚æœç¼ºå°‘å…³é”®ä¿¡æ¯ï¼ˆå¦‚ç›®æ ‡è¡¨ï¼‰ï¼Œå¿…é¡»è¦æ±‚ç”¨æˆ·è¡¥å……
   - confidence < 0.7 æ—¶ï¼Œå¿…é¡»æœ‰ ambiguities

## è¾“å‡ºæ ¼å¼

```json
{{
  "summary": "ä¸€å¥è¯æ¦‚æ‹¬éœ€æ±‚ï¼ˆå¿…é¡»å…·ä½“ï¼Œä¸èƒ½æ¨¡ç³Šï¼‰",
  "steps": [
    {{
      "step_id": "step_1",
      "step_name": "ä¸šåŠ¡æ­¥éª¤åç§°",
      "description": "è¿™ä¸€æ­¥åšä»€ä¹ˆï¼ˆä¸šåŠ¡æè¿°ï¼‰",
      "input_tables": ["schema.table"],
      "output_table": "schema.table",
      "depends_on": []
    }}
  ],
  "final_target": {{
    "table_name": "æœ€ç»ˆç›®æ ‡è¡¨ï¼ˆå¿…é¡»æ˜ç¡®ï¼‰",
    "write_mode": "overwrite",
    "partition_by": ["dt"]
  }},
  "ambiguities": [
    {{
      "question": "éœ€è¦ç”¨æˆ·æ¾„æ¸…çš„å…·ä½“é—®é¢˜",
      "context": "ä¸ºä»€ä¹ˆéœ€è¦æ¾„æ¸…",
      "options": ["å¯èƒ½çš„é€‰é¡¹1", "å¯èƒ½çš„é€‰é¡¹2"]
    }}
  ],
  "confidence": 0.85
}}
```

é‡è¦ï¼š
- å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç”¨æˆ·æåˆ°çš„æºå¤´è¾“å…¥è¡¨æˆ–æœ€ç»ˆç›®æ ‡è¡¨ï¼Œç›´æ¥è¿”å›æç¤ºï¼Œconfidence è®¾ä¸º 0
- å¦‚æœæ— æ³•æ˜ç¡® input_tables æˆ– output_tableï¼Œå¿…é¡»åœ¨ ambiguities ä¸­æé—®
- confidence åæ˜ éœ€æ±‚çš„æ˜ç¡®ç¨‹åº¦ï¼Œæ¨¡ç³Šéœ€æ±‚å¿…é¡» < 0.7

åªè¾“å‡º JSONï¼Œä¸è¦è§£é‡Šã€‚
"""


# ç»‘å®šçš„å·¥å…·
ANALYST_TOOLS = [search_assets, get_table_columns]


class AnalystAgent:
    """
    éœ€æ±‚åˆ†æå¸ˆ

    èŒè´£ï¼š
    1. åŸºäºçŸ¥è¯†åº“æ”¶æ•›ç”¨æˆ·éœ€æ±‚
    2. é€šè¿‡å·¥å…·éªŒè¯æ¶‰åŠçš„è¡¨æ˜¯å¦å­˜åœ¨
    3. éœ€æ±‚ä¸æ˜ç¡®æ—¶å¼ºåˆ¶è¦æ±‚æ¾„æ¸…
    4. ä¸å…è®¸æ¨¡ç³Šéœ€æ±‚å¾€åä¼ 
    """

    def __init__(self):
        self.llm = call_llm(temperature=0.0)
        self.llm_with_tools = self.llm.bind_tools(ANALYST_TOOLS)
        self.max_tool_calls = 4

    async def __call__(self, state: AgentState) -> Command:
        """æ‰§è¡Œéœ€æ±‚åˆ†æ"""
        user_query = state.user_input

        if not user_query:
            return Command(
                update={
                    "messages": [AIMessage(content="ç¼ºå°‘ç”¨æˆ·è¾“å…¥ï¼Œæ— æ³•åˆ†æéœ€æ±‚")],
                    "current_agent": "analyst_agent",
                    "error": "ç¼ºå°‘ç”¨æˆ·è¾“å…¥",
                }
            )

        logger.info(f"ğŸ“‹ AnalystAgent å¼€å§‹åˆ†æéœ€æ±‚: {user_query}")

        try:
            # è·å–ä¸Šä¸‹æ–‡
            global_kg_context = state.get_global_kg_context()
            agent_context = state.get_agent_context(AgentType.ANALYST)

            if not global_kg_context:
                global_kg_context = GlobalKGContext()

            if not agent_context:
                agent_context = AgentScopedContext.create_for_agent(
                    agent_type=AgentType.ANALYST,
                    tables=[],
                    user_query=user_query,
                )

            # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
            context_info = self._format_context(global_kg_context, agent_context)

            # æ‰§è¡Œåˆ†æï¼ˆå¸¦å·¥å…·è°ƒç”¨ï¼‰
            result_dict = await self._analyze_with_tools(
                user_query=user_query,
                context_info=context_info,
                agent_context=agent_context,
            )

            analysis_result = self._build_analysis_result(result_dict, user_query)

            # åŸºäºçŸ¥è¯†åº“éªŒè¯éœ€æ±‚æ˜¯å¦æ”¶æ•›
            validation_issues = await self._validate_convergence(
                analysis_result, global_kg_context, agent_context
            )
            if validation_issues:
                for issue in validation_issues:
                    analysis_result.ambiguities.append(issue)
                analysis_result.confidence = min(analysis_result.confidence, 0.5)

            plan_summary = analysis_result.get_execution_plan_summary()
            logger.info(f"âœ… AnalystAgent å®Œæˆåˆ†æ:\n{plan_summary}")

            # éœ€æ±‚ä¸æ˜ç¡®æ—¶ï¼Œå¼ºåˆ¶è¦æ±‚æ¾„æ¸…
            if analysis_result.needs_clarification() or analysis_result.confidence < 0.7:
                questions = [a.question for a in analysis_result.ambiguities]
                return Command(
                    update={
                        "messages": [AIMessage(content="éœ€æ±‚ä¸å¤Ÿæ˜ç¡®ï¼Œè¯·è¡¥å……ä»¥ä¸‹ä¿¡æ¯")],
                        "analysis_result": analysis_result.model_dump(),
                        "current_agent": "analyst_agent",
                        "needs_clarification": True,
                        "clarification_questions": questions,
                    }
                )

            return Command(
                update={
                    "messages": [AIMessage(content=f"éœ€æ±‚åˆ†æå®Œæˆ: {analysis_result.summary}")],
                    "analysis_result": analysis_result.model_dump(),
                    "current_agent": "analyst_agent",
                }
            )

        except Exception as e:
            logger.error(f"AnalystAgent åˆ†æå¤±è´¥: {e}", exc_info=True)
            return Command(
                update={
                    "messages": [AIMessage(content=f"éœ€æ±‚åˆ†æå¤±è´¥: {str(e)}")],
                    "current_agent": "analyst_agent",
                    "error": str(e),
                }
            )

    async def _analyze_with_tools(
        self,
        user_query: str,
        context_info: dict,
        agent_context: AgentScopedContext,
    ) -> dict:
        """æ‰§è¡Œå¸¦å·¥å…·è°ƒç”¨çš„åˆ†æ"""
        prompt = ANALYST_AGENT_PROMPT.format(
            discovered_tables=", ".join(agent_context.tables) if agent_context.tables else "ï¼ˆæ— ï¼‰",
            tables_summary=context_info["tables"],
            lineage_summary=context_info["lineage"],
            tools_description=agent_context.get_tools_description(),
            user_query=user_query,
        )

        messages = [HumanMessage(content=prompt)]
        tool_call_count = 0

        while tool_call_count < self.max_tool_calls:
            response = await self.llm_with_tools.ainvoke(messages)
            messages.append(response)

            # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè§£æå“åº”
            if not response.tool_calls:
                return self._parse_response(response.content)

            # æ‰§è¡Œå·¥å…·è°ƒç”¨
            for tool_call in response.tool_calls:
                tool_call_count += 1
                tool_name = tool_call["name"]
                tool_args = tool_call["args"]
                tool_id = tool_call["id"]

                logger.info(f"ğŸ”§ AnalystAgent è°ƒç”¨å·¥å…·: {tool_name}({tool_args})")

                tool_result = await self._execute_tool(tool_name, tool_args)

                messages.append(
                    ToolMessage(content=tool_result, tool_call_id=tool_id)
                )

                if tool_call_count >= self.max_tool_calls:
                    break

        # è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°ï¼Œè·å–æœ€ç»ˆå“åº”
        response = await self.llm.ainvoke(messages)
        return self._parse_response(response.content)

    async def _execute_tool(self, tool_name: str, tool_args: dict) -> str:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
        try:
            if tool_name == "search_assets":
                return await search_assets.ainvoke(tool_args)
            elif tool_name == "get_table_columns":
                return await get_table_columns.ainvoke(tool_args)
            else:
                return json.dumps({"status": "error", "message": f"æœªçŸ¥å·¥å…·: {tool_name}"})
        except Exception as e:
            logger.error(f"å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥: {e}")
            return json.dumps({"status": "error", "message": str(e)})

    async def _validate_convergence(
        self,
        analysis: AnalysisResult,
        global_kg_context: GlobalKGContext,
        agent_context: AgentScopedContext,
    ) -> list[Ambiguity]:
        """
        éªŒè¯éœ€æ±‚æ˜¯å¦æ”¶æ•›

        éªŒè¯è§„åˆ™ï¼ˆä¸šåŠ¡å±‚é¢ï¼‰ï¼š
        1. æ¯ä¸ª Step å¿…é¡»æœ‰æ˜ç¡®çš„ input_tables å’Œ output_table
        2. æºå¤´è¾“å…¥è¡¨ï¼ˆç¬¬ä¸€ä¸ª Step çš„ inputï¼‰å¿…é¡»åœ¨çŸ¥è¯†åº“ä¸­å­˜åœ¨
        3. æœ€ç»ˆç›®æ ‡è¡¨ï¼ˆfinal_targetï¼‰å¿…é¡»åœ¨çŸ¥è¯†åº“ä¸­å­˜åœ¨

        æ³¨æ„ï¼šä¸­é—´æ­¥éª¤çš„è¡¨ä¸éªŒè¯ï¼Œé‚£æ˜¯æŠ€æœ¯å±‚é¢çš„äº‹æƒ…ï¼ˆArchitectAgent è´Ÿè´£ï¼‰
        """
        issues = []

        # è·å–å·²çŸ¥è¡¨é›†åˆ
        known_tables = set(global_kg_context.get_table_names())
        known_tables.update(agent_context.tables)

        # 1. æ£€æŸ¥ Steps æ˜¯å¦å­˜åœ¨
        if not analysis.steps:
            issues.append(Ambiguity(
                question="æ— æ³•ä»éœ€æ±‚ä¸­è¯†åˆ«å‡ºå…·ä½“çš„ä¸šåŠ¡æ­¥éª¤ï¼Œè¯·æ˜ç¡®è¯´æ˜è¦åšä»€ä¹ˆ",
                context="éœ€æ±‚è¿‡äºæ¨¡ç³Šï¼Œæ— æ³•æ‹†åˆ†ä¸ºå¯æ‰§è¡Œçš„æ­¥éª¤",
                options=["è¯·æè¿°å…·ä½“çš„æ•°æ®å¤„ç†é€»è¾‘", "è¯·æŒ‡æ˜æºè¡¨å’Œç›®æ ‡è¡¨"],
            ))
            return issues

        # 2. æ£€æŸ¥æ¯ä¸ª Step æ˜¯å¦æœ‰è¾“å…¥è¾“å‡ºå£°æ˜
        for step in analysis.steps:
            if not step.input_tables:
                issues.append(Ambiguity(
                    question=f"æ­¥éª¤ '{step.step_name}' ç¼ºå°‘è¾“å…¥è¡¨ï¼Œè¯·æŒ‡æ˜ä»å“ªäº›è¡¨è¯»å–æ•°æ®",
                    context=f"æ­¥éª¤æè¿°: {step.description}",
                    options=[],
                ))
            if not step.output_table:
                issues.append(Ambiguity(
                    question=f"æ­¥éª¤ '{step.step_name}' ç¼ºå°‘è¾“å‡ºè¡¨ï¼Œè¯·æŒ‡æ˜æ•°æ®å†™å…¥å“ªä¸ªè¡¨",
                    context=f"æ­¥éª¤æè¿°: {step.description}",
                    options=[],
                ))

        # 3. éªŒè¯æºå¤´è¾“å…¥è¡¨ï¼ˆç¬¬ä¸€ä¸ª Step çš„ input_tablesï¼‰
        if analysis.steps and analysis.steps[0].input_tables:
            for table in analysis.steps[0].input_tables:
                if table not in known_tables:
                    exists = await self._verify_table_exists(table)
                    if not exists:
                        issues.append(Ambiguity(
                            question=f"æºè¡¨ '{table}' ä¸åœ¨çŸ¥è¯†åº“ä¸­ï¼Œè¯·ç¡®è®¤è¡¨åæ˜¯å¦æ­£ç¡®",
                            context="æºå¤´è¾“å…¥è¡¨å¿…é¡»åœ¨ Gravitino ä¸­å­˜åœ¨",
                            options=["è¯·æ£€æŸ¥è¡¨åæ‹¼å†™", "è¯·å…ˆåœ¨ Gravitino ä¸­åˆ›å»ºæ­¤è¡¨å¹¶åŒæ­¥å…ƒæ•°æ®"],
                        ))

        # 4. éªŒè¯æœ€ç»ˆç›®æ ‡è¡¨
        if not analysis.final_target or not analysis.final_target.table_name:
            issues.append(Ambiguity(
                question="è¯·æ˜ç¡®æœ€ç»ˆæ•°æ®è¦å†™å…¥å“ªä¸ªè¡¨",
                context="ç¼ºå°‘æœ€ç»ˆç›®æ ‡è¡¨ä¿¡æ¯",
                options=[],
            ))
        else:
            final_table = analysis.final_target.table_name
            if final_table not in known_tables:
                exists = await self._verify_table_exists(final_table)
                if not exists:
                    issues.append(Ambiguity(
                        question=f"æœ€ç»ˆç›®æ ‡è¡¨ '{final_table}' ä¸åœ¨çŸ¥è¯†åº“ä¸­",
                        context="æœ€ç»ˆç›®æ ‡è¡¨å¿…é¡»å…ˆåœ¨ Gravitino ä¸­åˆ›å»ºå¹¶åŒæ­¥å…ƒæ•°æ®",
                        options=["è¯·å…ˆåˆ›å»ºç›®æ ‡è¡¨ï¼ŒåŒæ­¥å…ƒæ•°æ®åå†ç”Ÿæˆ ETL"],
                    ))

        return issues

    async def _verify_table_exists(self, table_name: str) -> bool:
        """é€šè¿‡å·¥å…·éªŒè¯è¡¨æ˜¯å¦å­˜åœ¨"""
        try:
            result = await get_table_columns.ainvoke({"table_name": table_name})
            data = json.loads(result)
            return data.get("status") == "success"
        except Exception as e:
            logger.warning(f"éªŒè¯è¡¨ {table_name} æ˜¯å¦å­˜åœ¨å¤±è´¥: {e}")
            return False

    def _parse_response(self, content: str) -> dict:
        """è§£æ LLM å“åº”"""
        logger.info(f"ğŸ” è§£æ LLM å“åº” (é•¿åº¦: {len(content)}): {content[:1000]}...")

        json_match = re.search(r'```json\s*([\s\S]*?)\s*```', content)
        if json_match:
            return json.loads(json_match.group(1))

        json_match = re.search(r'\{[\s\S]*\}', content)
        if json_match:
            try:
                return json.loads(json_match.group())
            except json.JSONDecodeError as e:
                logger.error(f"JSON è§£æå¤±è´¥: {e}, åŒ¹é…å†…å®¹: {json_match.group()[:500]}...")

        logger.error(f"æ— æ³•è§£æ LLM å“åº”ä¸º JSONï¼ŒåŸå§‹å†…å®¹: {content}")
        raise ValueError("æ— æ³•è§£æ LLM å“åº”ä¸º JSON")

    def _build_analysis_result(self, result_dict: dict, user_query: str) -> AnalysisResult:
        """æ„å»º AnalysisResult"""
        steps = []
        for step_dict in result_dict.get("steps", []):
            step = Step(
                step_id=step_dict.get("step_id", ""),
                step_name=step_dict.get("step_name", ""),
                description=step_dict.get("description", ""),
                input_tables=step_dict.get("input_tables", []),
                output_table=step_dict.get("output_table"),
                depends_on=step_dict.get("depends_on", []),
            )
            steps.append(step)

        # è§£æ ambiguities
        ambiguities = []
        for amb_dict in result_dict.get("ambiguities", []):
            if isinstance(amb_dict, dict):
                ambiguities.append(Ambiguity(
                    question=amb_dict.get("question", ""),
                    context=amb_dict.get("context"),
                    options=amb_dict.get("options", []),
                ))
            elif isinstance(amb_dict, str):
                ambiguities.append(Ambiguity(question=amb_dict))

        # è§£æ final_target ä¸º DataTarget å¯¹è±¡
        final_target = None
        final_target_dict = result_dict.get("final_target")
        if final_target_dict and isinstance(final_target_dict, dict):
            final_target = DataTarget(
                table_name=final_target_dict.get("table_name", ""),
                write_mode=final_target_dict.get("write_mode", "overwrite"),
                partition_by=final_target_dict.get("partition_by", []),
                description=final_target_dict.get("description"),
            )

        return AnalysisResult(
            user_query=user_query,
            summary=result_dict.get("summary", ""),
            steps=steps,
            final_target=final_target,
            ambiguities=ambiguities,
            confidence=result_dict.get("confidence", 0.5),
        )

    def _format_context(
        self, global_kg_context: GlobalKGContext, agent_context: AgentScopedContext
    ) -> dict:
        """æ ¼å¼åŒ–ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        # æ ¼å¼åŒ–è¡¨ä¿¡æ¯
        tables_lines = []
        for catalog in global_kg_context.catalogs:
            for schema in catalog.schemas:
                for table in schema.tables[:20]:
                    tags_str = ", ".join(table.tags) if table.tags else ""
                    tables_lines.append(
                        f"- {schema.name}.{table.name} ({table.column_count}åˆ—) [{tags_str}]"
                    )

        # æ ¼å¼åŒ–è¡€ç¼˜
        lineage_lines = []
        for edge in global_kg_context.lineage_graph[:10]:
            lineage_lines.append(f"- {edge.source_table} â†’ {edge.target_table}")

        return {
            "tables": "\n".join(tables_lines) if tables_lines else "ï¼ˆæ— ï¼‰",
            "lineage": "\n".join(lineage_lines) if lineage_lines else "ï¼ˆæ— ï¼‰",
        }
