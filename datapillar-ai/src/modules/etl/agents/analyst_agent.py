"""
Analyst Agentï¼ˆéœ€æ±‚åˆ†æå¸ˆï¼‰

èŒè´£ï¼šä¸šåŠ¡å±‚é¢çš„éœ€æ±‚åˆ†æä¸æ”¶æ•›
- å°†ç”¨æˆ·éœ€æ±‚æ‹†åˆ†ä¸ºä¸šåŠ¡æ­¥éª¤ï¼ˆStepï¼‰
- åŸºäºçŸ¥è¯†åº“éªŒè¯éœ€æ±‚çš„å¯è¡Œæ€§
- éœ€æ±‚å¿…é¡»åœ¨æ­¤é˜¶æ®µæ”¶æ•›æ¸…æ¥šï¼Œä¸å…è®¸æ¨¡ç³Šéœ€æ±‚å¾€åä¼ 
- é€šè¿‡å·¥å…·éªŒè¯è¡¨æ˜¯å¦å­˜åœ¨
"""

import asyncio
import json
import logging
import time
from typing import Any

from langchain_core.messages import ToolMessage

from src.infrastructure.llm.client import call_llm
from src.infrastructure.resilience import get_resilience_config
from src.modules.etl.agents.knowledge_agent import AgentType, get_agent_tools
from src.modules.etl.agents.prompt_messages import build_llm_messages
from src.modules.etl.schemas.agent_result import AgentResult
from src.modules.etl.schemas.analyst import (
    AnalysisResult,
    AnalysisResultOutput,
)
from src.modules.etl.tools.table import get_table_detail

logger = logging.getLogger(__name__)


def _tool_error(message: str, **extra: object) -> str:
    """æ„é€ å·¥å…·é”™è¯¯å“åº”"""
    payload: dict[str, object] = {"status": "error", "message": message}
    payload.update(extra)
    return json.dumps(payload, ensure_ascii=False)


ANALYST_AGENT_SYSTEM_INSTRUCTIONS = """ä½ æ˜¯ Datapillar çš„éœ€æ±‚åˆ†æå¸ˆï¼ˆAnalystAgentï¼‰ã€‚

## ä½ çš„ä»»åŠ¡
å°†ç”¨æˆ·çš„ ETL éœ€æ±‚æ‹†åˆ†ä¸ºå¯æ‰§è¡Œçš„ä¸šåŠ¡æ­¥éª¤ï¼ˆStepï¼‰ï¼Œå¹¶éªŒè¯æ¶‰åŠçš„è¡¨æ˜¯å¦å­˜åœ¨ã€‚

## å¯ç”¨å·¥å…·

### get_table_detail
æŸ¥è¯¢è¡¨çš„è¯¦ç»†ä¿¡æ¯ï¼ˆå­—æ®µã€ç±»å‹ç­‰ï¼‰ã€‚
- ç”¨æˆ·æåˆ°çš„è¡¨åå¯èƒ½ä¸å®Œæ•´ï¼Œéœ€è¦é€šè¿‡æ­¤å·¥å…·éªŒè¯
- å¦‚æœè¿”å›"æœªæ‰¾åˆ°è¡¨"ï¼Œè¯´æ˜è¡¨åæˆ–è·¯å¾„ä¸æ­£ç¡®

## å·¥ä½œæµç¨‹
1. åˆ†æç”¨æˆ·éœ€æ±‚
2. å¦‚æœéœ€è¦éªŒè¯è¡¨ä¿¡æ¯ï¼Œè°ƒç”¨ get_table_detail
3. åˆ†æå®Œæˆåï¼Œç›´æ¥è¾“å‡º JSON æ ¼å¼çš„åˆ†æç»“æœ

## è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰
åˆ†æå®Œæˆåï¼Œç›´æ¥è¾“å‡ºä»¥ä¸‹ JSON æ ¼å¼ï¼š
```json
{
  "summary": "ä¸€å¥è¯æ¦‚æ‹¬ç”¨æˆ·éœ€æ±‚",
  "confidence": 0.8,
  "steps": [
    {
      "step_id": "s1",
      "step_name": "æ­¥éª¤åç§°",
      "description": "è¿™ä¸€æ­¥åšä»€ä¹ˆ",
      "input_tables": ["catalog.schema.table"],
      "output_table": "catalog.schema.table",
      "depends_on": []
    }
  ],
  "final_target": {
    "table_name": "ç›®æ ‡è¡¨å",
    "write_mode": "overwrite",
    "description": "æè¿°"
  },
  "ambiguities": []
}
```

## å­—æ®µè¯´æ˜
- summary: ä¸€å¥è¯æ¦‚æ‹¬ç”¨æˆ·éœ€æ±‚
- confidence: éœ€æ±‚æ˜ç¡®ç¨‹åº¦ (0-1)ï¼Œæ¨¡ç³Šéœ€æ±‚ < 0.7
- steps: ä¸šåŠ¡æ­¥éª¤åˆ—è¡¨
  - step_id: æ­¥éª¤å”¯ä¸€æ ‡è¯†
  - step_name: æ­¥éª¤åç§°
  - description: è¿™ä¸€æ­¥åšä»€ä¹ˆ
  - input_tables: è¾“å…¥è¡¨åˆ—è¡¨ï¼ˆå®Œæ•´è·¯å¾„ catalog.schema.tableï¼‰
  - output_table: è¾“å‡ºè¡¨ï¼ˆå®Œæ•´è·¯å¾„ï¼‰
  - depends_on: ä¾èµ–çš„ä¸Šæ¸¸æ­¥éª¤ ID
- final_target: æœ€ç»ˆæ•°æ®ç›®æ ‡
  - table_name: ç›®æ ‡è¡¨å
  - write_mode: overwrite/append/upsert
  - description: æè¿°
- ambiguities: éœ€è¦æ¾„æ¸…çš„é—®é¢˜åˆ—è¡¨

## æ”¶æ•›æ ‡å‡†
éœ€æ±‚åˆ†æå¿…é¡»"æ”¶æ•›"æ‰ç®—å®Œæˆï¼š
1. æ¯ä¸ª Step å¿…é¡»æœ‰æ˜ç¡®çš„ input_tables å’Œ output_table
2. å¿…é¡»æœ‰ final_target
3. confidence >= 0.7

å¦‚æœæ— æ³•æ”¶æ•›ï¼Œè®¾ç½® confidence < 0.7 å¹¶åœ¨ ambiguities ä¸­åˆ—å‡ºé—®é¢˜ã€‚

## é‡è¦çº¦æŸ
1. ä½ åªè´Ÿè´£"åšä»€ä¹ˆ"ï¼ˆä¸šåŠ¡æ‹†è§£ï¼‰ï¼Œä¸å†™ SQLï¼Œä¸é€‰ç»„ä»¶
2. ä¸å…è®¸è‡†é€ è¡¨åï¼Œå¿…é¡»é€šè¿‡å·¥å…·éªŒè¯æˆ–åœ¨ ambiguities ä¸­è¯¢é—®
3. åˆ†æå®Œæˆåç›´æ¥è¾“å‡º JSONï¼Œä¸è¦è°ƒç”¨ä»»ä½•å·¥å…·
"""


class AnalystAgent:
    """
    éœ€æ±‚åˆ†æå¸ˆ

    èŒè´£ï¼š
    1. åŸºäºçŸ¥è¯†åº“æ”¶æ•›ç”¨æˆ·éœ€æ±‚
    2. é€šè¿‡å·¥å…·éªŒè¯æ¶‰åŠçš„è¡¨æ˜¯å¦å­˜åœ¨
    3. éœ€æ±‚ä¸æ˜ç¡®æ—¶å¼ºåˆ¶è¦æ±‚æ¾„æ¸…
    4. ä¸å…è®¸æ¨¡ç³Šéœ€æ±‚å¾€åä¼ 
    """

    def __init__(self):
        self.llm = call_llm(temperature=0.0)
        config = get_resilience_config()
        self.max_iterations = config.max_iterations
        self.allowlist = get_agent_tools(AgentType.ANALYST)

    async def run(
        self,
        *,
        user_query: str,
        knowledge_agent=None,
        memory_context: dict[str, Any] | None = None,
    ) -> AgentResult:
        """
        æ‰§è¡Œéœ€æ±‚åˆ†æ

        å‚æ•°ï¼š
        - user_query: ç”¨æˆ·è¾“å…¥
        - knowledge_agent: KnowledgeAgent å®ä¾‹ï¼ˆç”¨äºæŒ‰éœ€æŸ¥è¯¢æŒ‡é’ˆï¼‰
        - memory_context: å¯¹è¯å†å²ä¸Šä¸‹æ–‡ï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰

        è¿”å›ï¼š
        - AgentResult: æ‰§è¡Œç»“æœ
        """
        self._knowledge_agent = knowledge_agent

        if not user_query:
            return AgentResult.failed(
                summary="ç¼ºå°‘ç”¨æˆ·è¾“å…¥",
                error="ç¼ºå°‘ç”¨æˆ·è¾“å…¥ï¼Œæ— æ³•åˆ†æéœ€æ±‚",
            )

        logger.info(f"ğŸ“‹ AnalystAgent å¼€å§‹åˆ†æéœ€æ±‚: {user_query}")

        try:
            llm_with_tools = self._bind_tools()

            output = await self._analyze_with_tools(
                user_query=user_query,
                llm_with_tools=llm_with_tools,
                memory_context=memory_context,
            )

            analysis_result = AnalysisResult.from_output(output, user_query)
            logger.info(f"âœ… AnalystAgent å®Œæˆåˆ†æ:\n{analysis_result.plan_summary()}")

            # æ£€æŸ¥ LLM è¿”å›çš„ confidence å’Œ ambiguitiesï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦ç”¨æˆ·æ¾„æ¸…
            if analysis_result.confidence < 0.7 and analysis_result.ambiguities:
                logger.info(
                    f"âš ï¸ AnalystAgent éœ€è¦æ¾„æ¸…: confidence={analysis_result.confidence}, "
                    f"ambiguities={analysis_result.ambiguities}"
                )
                return AgentResult.needs_clarification(
                    summary="éœ€æ±‚ä¸å¤Ÿæ˜ç¡®ï¼Œéœ€è¦æ¾„æ¸…",
                    message="æˆ‘æœ‰ä¸€äº›é—®é¢˜éœ€è¦ç¡®è®¤åæ‰èƒ½ç»§ç»­åˆ†æ",
                    questions=analysis_result.ambiguities,
                )

            return AgentResult.completed(
                summary=f"éœ€æ±‚åˆ†æå®Œæˆ: {analysis_result.summary}",
                deliverable=analysis_result,
                deliverable_type="analysis",
            )

        except Exception as e:
            logger.error(f"AnalystAgent åˆ†æå¤±è´¥: {e}", exc_info=True)
            return AgentResult.failed(
                summary=f"éœ€æ±‚åˆ†æå¤±è´¥: {str(e)}",
                error=str(e),
            )

    async def _analyze_with_tools(
        self,
        user_query: str,
        llm_with_tools,
        memory_context: dict[str, Any] | None = None,
    ) -> AnalysisResultOutput:
        """
        å¸¦å·¥å…·è°ƒç”¨çš„åˆ†ææµç¨‹ï¼š
        1. é¢„å…ˆè°ƒç”¨ KnowledgeAgent è·å–å€™é€‰è¡¨/åˆ—/å€¼åŸŸï¼ˆå¸¦æƒé™è¿‡æ»¤ï¼‰
        2. ç¬¬ä¸€é˜¶æ®µï¼šLLM è°ƒç”¨å·¥å…·æ”¶é›†ä¿¡æ¯ï¼ˆbind_tools + ToolMessageï¼‰
        3. ç¬¬äºŒé˜¶æ®µï¼šLLM è¾“å‡ºç»“æ„åŒ–ç»“æœï¼ˆwith_structured_output + parse_structured_output å…œåº•ï¼‰
        """
        total_start = time.perf_counter()

        # é¢„å…ˆæ£€ç´¢çŸ¥è¯†ä¸Šä¸‹æ–‡ï¼ˆå¸¦æƒé™è¿‡æ»¤ï¼‰
        context_payload = None
        if self._knowledge_agent:
            search_start = time.perf_counter()
            ctx = await self._knowledge_agent.global_search(user_query, top_k=10, min_score=0.5)
            search_elapsed = time.perf_counter() - search_start
            logger.info(f"â±ï¸ çŸ¥è¯†æ£€ç´¢è€—æ—¶: {search_elapsed:.2f}s, æ‰¾åˆ° {ctx.summary()}")
            # ä¼ å…¥ allowlist è¿‡æ»¤é’¥åŒ™ï¼šåªä¿ç•™è¯¥å‘˜å·¥æœ‰æƒé™çš„å·¥å…·
            context_payload = ctx.to_llm_context(allowlist=self.allowlist)

        messages = build_llm_messages(
            system_instructions=ANALYST_AGENT_SYSTEM_INSTRUCTIONS,
            agent_id="analyst_agent",
            user_query=user_query,
            context_payload=context_payload,
            memory_context=memory_context,
        )

        # ç¬¬ä¸€é˜¶æ®µï¼šå·¥å…·è°ƒç”¨æ”¶é›†ä¿¡æ¯
        for iteration in range(1, self.max_iterations + 1):
            llm_start = time.perf_counter()
            response = await llm_with_tools.ainvoke(messages)
            llm_elapsed = time.perf_counter() - llm_start
            logger.info(f"â±ï¸ [ç¬¬{iteration}è½®] LLM è°ƒç”¨è€—æ—¶: {llm_elapsed:.2f}s")

            if not response.tool_calls:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿›å…¥ç¬¬äºŒé˜¶æ®µ
                break

            # æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼Œç»“æœæ”¾å…¥ ToolMessage
            messages.append(response)
            for tc in response.tool_calls:
                logger.info(f"ğŸ”§ AnalystAgent è°ƒç”¨å·¥å…·: {tc['name']}({tc['args']})")

            tool_start = time.perf_counter()
            results = await asyncio.gather(
                *[self._execute_tool(tc["name"], tc["args"]) for tc in response.tool_calls]
            )
            tool_elapsed = time.perf_counter() - tool_start
            logger.info(
                f"â±ï¸ [ç¬¬{iteration}è½®] å·¥å…·è°ƒç”¨è€—æ—¶: {tool_elapsed:.2f}s ({len(results)} ä¸ªå·¥å…·å¹¶è¡Œ)"
            )

            for tc, result in zip(response.tool_calls, results, strict=True):
                messages.append(ToolMessage(content=result, tool_call_id=tc["id"]))

        # ç¬¬äºŒé˜¶æ®µï¼šç»“æ„åŒ–è¾“å‡ºï¼ˆwith_structured_output è®© LLM çŸ¥é“ schemaï¼‰
        structured_start = time.perf_counter()
        output = await self._get_structured_output(messages, AnalysisResultOutput)
        structured_elapsed = time.perf_counter() - structured_start
        logger.info(f"â±ï¸ ç»“æ„åŒ–è¾“å‡ºè€—æ—¶: {structured_elapsed:.2f}s")

        total_elapsed = time.perf_counter() - total_start
        logger.info(f"â±ï¸ AnalystAgent æ€»è€—æ—¶: {total_elapsed:.2f}s")

        return output

    async def _get_structured_output(
        self,
        messages: list,
        schema: type[AnalysisResultOutput],
    ) -> AnalysisResultOutput:
        """
        è·å–ç»“æ„åŒ–è¾“å‡ºï¼šwith_structured_output(json_mode) + parse_structured_output å…œåº•
        """
        from src.infrastructure.llm.structured_output import parse_structured_output

        # ä½¿ç”¨ json_modeï¼ˆä¸æ˜¯ function_callingï¼Œé¿å…å’Œå·¥å…·è°ƒç”¨æ··æ·†ï¼‰
        llm_structured = self.llm.with_structured_output(
            schema,
            method="json_mode",
            include_raw=True,
        )
        result = await llm_structured.ainvoke(messages)

        # æƒ…å†µ 1ï¼šç›´æ¥è§£ææˆåŠŸ
        if isinstance(result, schema):
            return result

        # æƒ…å†µ 2ï¼šdict æ ¼å¼ï¼ˆinclude_raw=True çš„è¿”å›ï¼‰
        if isinstance(result, dict):
            parsed = result.get("parsed")
            if isinstance(parsed, schema):
                return parsed

            # è§£æå¤±è´¥ï¼Œå°è¯•ä» raw ä¸­æ¢å¤
            parsing_error = result.get("parsing_error")
            raw = result.get("raw")

            if raw:
                raw_text = getattr(raw, "content", None)
                if raw_text:
                    logger.warning(
                        "with_structured_output è§£æå¤±è´¥ï¼Œå°è¯• parse_structured_output å…œåº•"
                    )
                    try:
                        return parse_structured_output(raw_text, schema)
                    except ValueError as e:
                        logger.error(f"parse_structured_output å…œåº•ä¹Ÿå¤±è´¥: {e}")
                        raise

            if parsing_error:
                raise parsing_error

        raise ValueError(f"æ— æ³•è·å–ç»“æ„åŒ–è¾“å‡º: {type(result)}")

    async def _execute_tool(self, tool_name: str, tool_args: dict) -> str:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ˆæ”¯æŒç²¾ç¡®å‚æ•°å’Œæ¨¡ç³Šå‚æ•°ï¼‰"""
        try:
            if tool_name not in self.allowlist:
                return _tool_error(f"å·¥å…·ä¸åœ¨ allowlist ä¸­: {tool_name}")

            if tool_name == "get_table_detail":
                # æ£€æŸ¥æ˜¯å¦å·²æä¾›ç²¾ç¡®å‚æ•°
                catalog = tool_args.get("catalog")
                schema_name = tool_args.get("schema_name") or tool_args.get("schema")
                table = tool_args.get("table")

                # å¦‚æœåªæä¾›äº† table_nameï¼Œå°è¯•é€šè¿‡ knowledge_agent æŸ¥æ‰¾ç²¾ç¡®è·¯å¾„
                if not (catalog and schema_name and table):
                    table_name = tool_args.get("table_name") or tool_args.get("table") or ""
                    if not table_name:
                        return _tool_error("ç¼ºå°‘ table å‚æ•°")

                    # å°è¯•è§£æ schema.table æˆ– catalog.schema.table æ ¼å¼
                    parts = table_name.split(".")
                    if len(parts) >= 3:
                        catalog, schema_name, table = parts[0], parts[1], parts[2]
                    elif len(parts) == 2:
                        schema_name, table = parts[0], parts[1]
                        catalog = ""
                    else:
                        # æ— æ³•è§£æï¼Œå°è¯•é€šè¿‡ knowledge_agent æŸ¥æ‰¾
                        if self._knowledge_agent:
                            ctx = await self._knowledge_agent.global_search(
                                table_name, top_k=1, min_score=0.6
                            )
                            if ctx.tables:
                                pointer = ctx.tables[0]
                                catalog = pointer.catalog
                                schema_name = pointer.schema_name
                                table = pointer.table
                            else:
                                return _tool_error(f"æœªæ‰¾åˆ°è¡¨: {table_name}")
                        else:
                            return _tool_error(f"æ— æ³•è§£æè¡¨å: {table_name}")

                logger.info(
                    f"ğŸ”§ è°ƒç”¨å·¥å…·: {tool_name}(catalog={catalog}, schema_name={schema_name}, table={table})"
                )
                return await get_table_detail.ainvoke(
                    {
                        "catalog": catalog,
                        "schema_name": schema_name,
                        "table": table,
                    }
                )

            return _tool_error(f"æœªçŸ¥å·¥å…·: {tool_name}")
        except Exception as e:
            logger.error(f"å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥: {e}")
            return _tool_error(str(e))

    def _bind_tools(self):
        """ç»‘å®šæŸ¥è¯¢å·¥å…·åˆ° LLM"""
        return self.llm.bind_tools([get_table_detail])
