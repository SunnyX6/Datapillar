import json
import asyncio
import copy
from typing import Dict, Any, List
import logging

logger = logging.getLogger(__name__)
from langchain_core.messages import AIMessage
from langgraph.graph import StateGraph, END
from langgraph.types import Command

from src.agent.state import OrchestratorState
from src.agent.schemas import WorkflowOutput
from src.agent.utils import extract_json_from_text
from src.integrations.llm import call_llm


class CoderAgent:
    """
    CoderAgent - ä»£ç ç”Ÿæˆä¸“å®¶ï¼ˆWorkerï¼‰
    
    ã€æ ¸å¿ƒç‰¹æ€§ã€‘
    1. å¹¶å‘åŠ é€Ÿï¼šä½¿ç”¨ asyncio.gather å¹¶è¡Œå¤„ç†æ‰€æœ‰èŠ‚ç‚¹çš„é…ç½®ç”Ÿæˆï¼Œè€Œéä¸²è¡Œç­‰å¾…ã€‚
    2. èŒè´£è§£è€¦ï¼šSlotå¡«å……ã€æ‹“æ‰‘æ„å»ºã€Promptæ„å»ºåˆ†ç¦»ã€‚
    3. æ ‡å‡†æµæ§ï¼šä½¿ç”¨ Command æ§åˆ¶ LangGraph æµç¨‹ã€‚
    """

    def __init__(self):
        # åˆå§‹åŒ– LLMï¼Œå¼€å¯ JSON æ¨¡å¼å¢å¼ºç¨³å®šæ€§
        self.llm = call_llm(temperature=0.1, enable_json_mode=True)

    async def __call__(self, state: OrchestratorState) -> Command:
        """
        Worker æ ¸å¿ƒå…¥å£
        """
        logger.info("ğŸ’» CoderAgent: å¼€å§‹ç”Ÿæˆå·¥ä½œæµé…ç½®...")

        # 1. è·å–å¹¶è§„èŒƒåŒ– Plan æ•°æ®
        plan = getattr(state, "plan", None)
        if not plan:
            return self._handle_error("æœªæ‰¾åˆ°æ‰§è¡Œè®¡åˆ’ (state.plan ä¸ºç©º)")
        
        # å…¼å®¹ Pydantic å¯¹è±¡æˆ– Dict
        plan_data = plan if isinstance(plan, dict) else plan.model_dump()
        
        # âš ï¸ æ·±æ‹·è´ï¼šå› ä¸ºæˆ‘ä»¬è¦ä¿®æ”¹ nodes å†…éƒ¨ç»“æ„ï¼Œä¸è¦æ±¡æŸ“åŸå§‹ plan è®°å½•
        workflow_data = copy.deepcopy(plan_data)
        nodes = workflow_data.get("nodes", [])
        edges = workflow_data.get("edges", [])

        try:
            # =========================================================
            # ğŸ”¥ æ ¸å¿ƒä¼˜åŒ–ï¼šå¹¶å‘å¡«å……æ‰€æœ‰èŠ‚ç‚¹çš„ Slot
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åªæ”¶é›†ä»»åŠ¡ï¼Œä¸ç«‹å³ awaitï¼Œä»è€Œå®ç°å¹¶å‘
            # =========================================================
            tasks = []
            
            # ç­›é€‰å‡ºéœ€è¦ LLM å¡«å……çš„èŠ‚ç‚¹ (å¸¦ __slot__: true çš„èŠ‚ç‚¹)
            for node in nodes:
                config = node.get("data", {}).get("config", {})
                if config.get("__slot__"):
                    # åˆ›å»ºåç¨‹ä»»åŠ¡å¹¶åŠ å…¥åˆ—è¡¨
                    tasks.append(self._process_single_node(node))
            
            if tasks:
                logger.info(f"ğŸš€ [å¹¶å‘å¯åŠ¨] æ­£åœ¨å¹¶è¡Œå¤„ç† {len(tasks)} ä¸ªèŠ‚ç‚¹çš„é…ç½®ç”Ÿæˆ...")
                # ğŸ”¥ å¹¶å‘æ‰§è¡Œæ‰€æœ‰ LLM è°ƒç”¨ï¼Œç­‰å¾…å…¨éƒ¨å®Œæˆ
                # ç›¸æ¯”ä¸²è¡Œå¾ªç¯ï¼Œè¿™é‡Œçš„æ—¶é—´æ¶ˆè€— = æœ€æ…¢çš„é‚£ä¸ªèŠ‚ç‚¹è€—æ—¶ï¼Œè€Œä¸æ˜¯æ€»å’Œ
                await asyncio.gather(*tasks)
            else:
                logger.info("â„¹ï¸ æ²¡æœ‰å‘ç°éœ€è¦å¡«å……çš„ Slotï¼Œç›´æ¥ä½¿ç”¨åŸå§‹é…ç½®")
            
            # =========================================================
            # 3. æ„å»ºæœ€ç»ˆæ‹“æ‰‘ç»“æ„ï¼ˆæ·»åŠ  Start/End èŠ‚ç‚¹å¹¶è¿æ¥ï¼‰
            # =========================================================
            final_nodes, final_edges = self._build_topology(nodes, edges)

            # 4. ç»„è£…æœ€ç»ˆè¾“å‡ºå¯¹è±¡ (Pydantic æ ¡éªŒ)
            workflow_output = WorkflowOutput(
                workflowName=workflow_data.get("workflowName", "Generated Workflow"),
                taskType="ETL",
                description=workflow_data.get("description", ""),
                nodes=final_nodes,
                edges=final_edges,
            )

            logger.info(f"âœ… å·¥ä½œæµç”Ÿæˆå®Œæˆ: {workflow_output.workflowName}")

            # 5. è¿”å› Command (æ˜¾å¼ç»“æŸå­å›¾)
            return Command(
                update={
                    "messages": [AIMessage(content=f"ä»£ç ç”Ÿæˆå®Œæˆï¼Œå·¥ä½œæµå·²å°±ç»ªï¼š{workflow_output.workflowName}")],
                    # è¿™é‡Œè½¬ä¸º dict å­˜å…¥ state
                    "workflow": workflow_output.model_dump(mode="json"),
                    "current_agent": "coder_agent",
                    "is_found": True
                },
                goto=END  # æ˜ç¡®å‘Šè¯‰çˆ¶å›¾ï¼šCoder ä»»åŠ¡ç»“æŸ
            )

        except Exception as e:
            logger.exception(f"âŒ CoderAgent è¿è¡Œå¼‚å¸¸: {e}")
            return self._handle_error(f"å·¥ä½œæµç”Ÿæˆå‘ç”Ÿç³»ç»Ÿé”™è¯¯: {str(e)}")

    async def _process_single_node(self, node: Dict[str, Any]):
        """
        [åŸå­ä»»åŠ¡] å¤„ç†å•ä¸ªèŠ‚ç‚¹ï¼šç”Ÿæˆ Prompt -> è°ƒç”¨ LLM -> æ›´æ–° Node Config
        æ³¨æ„ï¼šæ­¤æ–¹æ³•ä¼šè¢«å¹¶å‘è°ƒç”¨ï¼Œä¿®æ”¹çš„æ˜¯ node å¯¹è±¡çš„å¼•ç”¨
        """
        node_id = node["id"]
        node_type = node["type"]
        config = node["data"]["config"]
        context_hints = config.get("__context_hints__", {})

        # æ„é€  Prompt
        prompt = self._build_prompt(node, config, context_hints)

        try:
            # â³ è€—æ—¶æ“ä½œï¼šè°ƒç”¨ LLM
            response = await self.llm.ainvoke(prompt)
            
            # è§£æ JSON
            generated_json = extract_json_from_text(response.content)
            filled_config = json.loads(generated_json)

            # ğŸ§¹ æ¸…ç†æ ‡è®°å­—æ®µ (é˜²æ­¢æ±¡æŸ“å‰ç«¯)
            filled_config.pop("__slot__", None)
            filled_config.pop("__context_hints__", None)

            # ğŸ”„ æ›´æ–°èŠ‚ç‚¹é…ç½® (å¼•ç”¨ä¿®æ”¹)
            node["data"]["config"] = filled_config
            # logger.debug(f"âœ¨ èŠ‚ç‚¹ {node_id} ({node_type}) å¡«å……å®Œæ¯•")

        except Exception as e:
            logger.error(f"âŒ èŠ‚ç‚¹ {node_id} é…ç½®ç”Ÿæˆå¤±è´¥: {e}")
            # å…œåº•ç­–ç•¥ï¼šç§»é™¤ slot æ ‡è®°ï¼Œä¿ç•™é”™è¯¯ä¿¡æ¯ï¼Œé˜²æ­¢å‰ç«¯æ­»å¾ªç¯åŠ è½½
            node["data"]["config"]["__slot__"] = False
            node["data"]["config"]["__error__"] = f"ç”Ÿæˆå¤±è´¥: {str(e)}"

    def _build_topology(self, nodes: List[Dict], edges: List[Dict]):
        """
        è¾…åŠ©æ–¹æ³•ï¼šæ ‡å‡†åŒ–æ‹“æ‰‘ï¼Œæ·»åŠ  Start/End èŠ‚ç‚¹
        """
        start_node = {
            "id": "node_start_sys", 
            "type": "start", 
            "position": {"x": 50, "y": 200}, 
            "data": {"label": "å¼€å§‹"}
        }
        end_node = {
            "id": "node_end_sys", 
            "type": "end", 
            "position": {"x": 1200, "y": 200}, 
            "data": {"label": "ç»“æŸ"}
        }

        # è¿‡æ»¤æ‰å¯èƒ½å·²å­˜åœ¨çš„ start/end (é¿å…é‡å¤æ·»åŠ )
        biz_nodes = [n for n in nodes if n["type"] not in ("start", "end")]
        
        if not biz_nodes:
            # ç©ºæµç¨‹å…œåº•
            return [start_node, end_node], [{"id": "link_start_end", "source": start_node["id"], "target": end_node["id"]}]

        first_id = biz_nodes[0]["id"]
        last_id = biz_nodes[-1]["id"]

        # ç»„è£…èŠ‚ç‚¹åˆ—è¡¨
        final_nodes = [start_node] + biz_nodes + [end_node]
        
        # ç»„è£…è¿çº¿
        # Start -> ç¬¬ä¸€ä¸ªä¸šåŠ¡èŠ‚ç‚¹
        edges.insert(0, {
            "id": f"link_start_{first_id}",
            "source": start_node["id"],
            "target": first_id
        })
        # æœ€åä¸€ä¸ªä¸šåŠ¡èŠ‚ç‚¹ -> End
        edges.append({
            "id": f"link_{last_id}_end",
            "source": last_id,
            "target": end_node["id"]
        })

        return final_nodes, edges

    def _build_prompt(self, node, config, hints):
        """æ„é€  Prompt æ¨¡æ¿"""
        # æ˜¾å¼åºåˆ—åŒ–ä¸ºå­—ç¬¦ä¸²ï¼Œç¡®ä¿ prompt æ ¼å¼æ­£ç¡®
        config_str = json.dumps(config, indent=2, ensure_ascii=False)
        hints_str = json.dumps(hints, indent=2, ensure_ascii=False)
        
        return f"""
        ä½ æ˜¯ Data AI Builder çš„é…ç½®ç”Ÿæˆä¸“å®¶ã€‚
        è¯·æ ¹æ®ä¸Šä¸‹æ–‡ä¸ºã€{node['type']}ã€‘èŠ‚ç‚¹ç”Ÿæˆå®Œæ•´çš„ JSON é…ç½®ã€‚

        ## èŠ‚ç‚¹ä¿¡æ¯
        - ID: {node['id']}
        - ç±»å‹: {node['type']}
        - æ ‡ç­¾: {node['data'].get('label', 'æœªå‘½å')}

        ## å¾…å¡«å……çš„é…ç½®æ¨¡æ¿

        {config_str}

        ## ä¸šåŠ¡ä¸Šä¸‹æ–‡ (Context Hints)
        {hints_str}
        ## è¦æ±‚
        åŸºäºä¸Šä¸‹æ–‡å¡«å……ï¼šåˆ©ç”¨ Context Hints ä¸­çš„è¡¨åã€å­—æ®µæ˜ å°„ã€SQLé€»è¾‘å¡«å……æ¨¡æ¿ä¸­çš„ç©ºå€¼ã€‚

        ä¿æŒç»“æ„ï¼šè¾“å‡ºçš„ JSON ç»“æ„å¿…é¡»ä¸æ¨¡æ¿å®Œå…¨ä¸€è‡´ã€‚

        æ¸…ç†æ•°æ®ï¼šè¾“å‡ºç»“æœä¸­ä¸è¦åŒ…å« __slot__ å’Œ __context_hints__ å­—æ®µã€‚

        æ ¼å¼ä¸¥æ ¼ï¼šåªè¾“å‡ºæ ‡å‡†çš„ JSON å­—ç¬¦ä¸²ï¼Œä¸è¦ Markdown ä»£ç å—ã€‚
        """
    def _handle_error(self, msg: str) -> Command: 
        """ç»Ÿä¸€é”™è¯¯å¤„ç†è¿”å›""" 
        return Command(update={ "messages": [AIMessage(content=msg)], "is_found": False }, goto=END)

    def build_coder_subgraph(): 
        """æ„å»ºå­å›¾""" 
        builder = StateGraph(OrchestratorState)
        builder.add_node("coder_llm", CoderAgent()) 
        builder.set_entry_point("coder_llm") 
        # å› ä¸º CoderAgent è¿”å›äº† Command(goto=END)ï¼Œè¿™é‡Œåªéœ€è¦å®šä¹‰èŠ‚ç‚¹ , ä½†ä¸ºäº†å›¾ç»“æ„çš„å®Œæ•´æ€§ï¼Œæ˜¾å¼æ·»åŠ è¾¹æ˜¯å¥½ä¹ æƒ¯ 
        builder.add_edge("coder_llm", END) 

        return builder.compile()