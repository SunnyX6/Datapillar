# Datapillar OneAgentic 配置示例
# 将此文件保存为 datapillar.toml 或 config.toml

# ============================================================================
# LLM 配置
# ============================================================================
[llm]
# 基础配置
# api_key = "sk-xxx"  # 敏感信息建议用环境变量: DATAPILLAR_LLM__API_KEY
# provider: 支持 openai, anthropic, glm, deepseek, openrouter, ollama
provider = "openai"
model = "gpt-4o"
# base_url = "https://api.openai.com/v1"  # 可选，使用自定义端点时设置
temperature = 0.0
timeout_seconds = 120

# --- 其他 Provider 配置示例 ---
# GLM (智谱):
#   provider = "glm"
#   model = "glm-4.7"
#   base_url = "https://open.bigmodel.cn/api/paas/v4"
#
# Anthropic (Claude):
#   provider = "anthropic"
#   model = "claude-3-opus-20240229"
#
# DeepSeek:
#   provider = "deepseek"
#   model = "deepseek-chat"
#   base_url = "https://api.deepseek.com/v1"
#
# OpenRouter:
#   provider = "openrouter"
#   model = "anthropic/claude-3-opus"
#   base_url = "https://openrouter.ai/api/v1"
#
# Ollama (本地):
#   provider = "ollama"
#   model = "llama3"
#   base_url = "http://localhost:11434/v1"

# 重试配置
retry.max_retries = 3
retry.initial_delay_ms = 500
retry.max_delay_ms = 30000
retry.exponential_base = 2.0
retry.jitter = true

# 熔断配置
circuit_breaker.failure_threshold = 5
circuit_breaker.recovery_seconds = 60

# 限流配置
rate_limit.enabled = true
rate_limit.default.rpm = 60
rate_limit.default.max_concurrent = 10

# 按 Provider 覆盖（可选）
# rate_limit.providers.glm.rpm = 10
# rate_limit.providers.glm.max_concurrent = 3

# LLM 响应缓存配置
[llm.cache]
enabled = true
backend = "memory"  # "memory" 或 "redis"
ttl_seconds = 300
max_size = 1000

# --- Redis 缓存配置示例 ---
# backend = "redis"
# redis_url = "redis://localhost:6379/0"
# key_prefix = "llm_cache:"

# ============================================================================
# Embedding 配置（经验学习/知识检索需要）
# ============================================================================
[embedding]
# 基础配置
# api_key = "sk-xxx"  # 敏感信息建议用环境变量: DATAPILLAR_EMBEDDING__API_KEY
# provider: 支持 openai, glm
provider = "openai"
model = "text-embedding-3-small"
# base_url = "https://api.openai.com/v1"  # 可选，使用自定义端点时设置
dimension = 1536

# --- 其他 Provider 配置示例 ---
# GLM (智谱):
#   provider = "glm"
#   model = "embedding-3"
#   dimension = 2048

# ============================================================================
# Agent 配置
# ============================================================================
[agent]
max_steps = 50
timeout_seconds = 300
tool_timeout_seconds = 30
checkpoint_ttl_seconds = 604800  # 7天

# Checkpointer 配置（状态持久化）
[agent.checkpointer]
type = "memory"  # memory | sqlite | postgres | redis
# path = "./data/checkpoint.db"  # sqlite 专用
# url = "redis://localhost:6379"  # postgres/redis 专用
# ttl_minutes = 10080  # redis 专用

# DeliverableStore 配置（Agent 交付物存储）
[agent.deliverable_store]
type = "memory"  # memory | postgres | redis
# url = "redis://localhost:6379"  # postgres/redis 专用

# ============================================================================
# VectorStore 配置（经验/知识共用）
# ============================================================================
[vector_store]
type = "lance"  # lance | chroma | milvus
path = "./data/vectors"  # lance/chroma 本地模式
# uri = "./data/milvus.db"  # milvus 本地模式
# uri = "http://localhost:19530"  # milvus 远程模式
# token = "root:Milvus"  # milvus 远程认证
# host = "localhost"  # chroma 远程模式
# port = 8000  # chroma 远程端口

# ============================================================================
# 经验学习配置
# ============================================================================
[learning]
verify_threshold = 0.7
reject_threshold = 0.3
retrieval_k = 5

# ============================================================================
# 知识配置
# ============================================================================
[knowledge.chunk]
mode = "general"  # general | parent_child | qa
preprocess = ["normalize_newlines", "remove_control", "collapse_whitespace"]

[knowledge.chunk.general]
max_tokens = 800
overlap = 120
# delimiter = "\n\n"  # 可选固定分隔符

[knowledge.retrieve]
method = "hybrid"  # semantic | hybrid
top_k = 8
# score_threshold = 0.2

[knowledge.retrieve.tuning]
pool_k = 32
rrf_k = 60

[knowledge.retrieve.quality]
dedupe = true
dedupe_threshold = 0.92
max_per_document = 2

[knowledge.retrieve.rerank]
mode = "off"  # off | model | weighted
provider = "sentence_transformers"
model = "cross-encoder/ms-marco-MiniLM-L-6-v2"
score_mode = "rank"  # rank | normalize | raw

[knowledge.retrieve.rerank.params]
# batch_size = 16
# device = "cuda"

[knowledge.retrieve.inject]
# mode=system: 每次 Agent 执行前自动检索并注入 SystemMessage
# mode=tool: 暴露 knowledge_retrieve 工具，Agent 按需调用检索（上下文更省）
# 注意：MapReduce reducer 不注入知识，仅汇总 Map 结果
mode = "tool"  # system | tool
max_tokens = 1200
max_chunks = 6
format = "markdown"  # markdown | json
