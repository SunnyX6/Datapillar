# Datapillar OneAgentic config example
# Save this file as datapillar.toml or config.toml

# ============================================================================
# LLM configuration
# ============================================================================
[llm]
# Base settings
# api_key = "sk-xxx"  # Sensitive data should use env var: DATAPILLAR_LLM_API_KEY
# provider: supports openai, anthropic, glm, deepseek, openrouter, ollama
provider = "openai"
model = "gpt-4o"
# base_url = "https://api.openai.com/v1"  # Optional, set for custom endpoint
temperature = 0.0
timeout_seconds = 120

# --- Other provider examples ---
# GLM (Zhipu):
#   provider = "glm"
#   model = "glm-4.7"
#   base_url = "https://open.bigmodel.cn/api/paas/v4"
#
# Anthropic (Claude):
#   provider = "anthropic"
#   model = "claude-3-opus-20240229"
#
# DeepSeek:
#   provider = "deepseek"
#   model = "deepseek-chat"
#   base_url = "https://api.deepseek.com/v1"
#
# OpenRouter:
#   provider = "openrouter"
#   model = "anthropic/claude-3-opus"
#   base_url = "https://openrouter.ai/api/v1"
#
# Ollama (local):
#   provider = "ollama"
#   model = "llama3"
#   base_url = "http://localhost:11434/v1"

# Retry settings
retry.max_retries = 3
retry.initial_delay_ms = 500
retry.max_delay_ms = 30000
retry.exponential_base = 2.0
retry.jitter = true

# Circuit breaker settings
circuit_breaker.failure_threshold = 5
circuit_breaker.recovery_seconds = 60

# Rate limiting
rate_limit.enabled = true
rate_limit.default.rpm = 60
rate_limit.default.max_concurrent = 10

# Provider overrides (optional)
# rate_limit.providers.glm.rpm = 10
# rate_limit.providers.glm.max_concurrent = 3

# LLM response cache
[llm.cache]
enabled = true
backend = "memory"  # "memory" or "redis"
ttl_seconds = 300
max_size = 1000

# --- Redis cache example ---
# backend = "redis"
# redis_url = "redis://localhost:6379/0"
# key_prefix = "llm_cache:"

# ============================================================================
# Embedding configuration (required for learning/knowledge retrieval)
# ============================================================================
[embedding]
# Base settings
# api_key = "sk-xxx"  # Sensitive data should use env var: DATAPILLAR_EMBEDDING_API_KEY
# provider: supports openai, glm
provider = "openai"
model = "text-embedding-3-small"
# base_url = "https://api.openai.com/v1"  # Optional, set for custom endpoint
dimension = 1536

# --- Other provider examples ---
# GLM (Zhipu):
#   provider = "glm"
#   model = "embedding-3"
#   dimension = 2048

# ============================================================================
# Agent configuration
# ============================================================================
[agent]
max_steps = 50
timeout_seconds = 300
tool_timeout_seconds = 30

# Checkpointer (state persistence)
[agent.checkpointer]
type = "memory"  # memory | sqlite | postgres | redis
# path = "./data/checkpoint.db"  # sqlite only
# url = "redis://localhost:6379"  # postgres/redis only
# ttl_minutes = 10080  # redis only (7 days)

# DeliverableStore (agent outputs)
[agent.deliverable_store]
type = "memory"  # memory | postgres | redis
# url = "redis://localhost:6379"  # postgres/redis only

# ============================================================================
# VectorStore configuration (shared by learning and knowledge)
# ============================================================================
[vector_store]
type = "lance"  # lance | chroma | milvus
path = "./data/vectors"  # lance/chroma local
# uri = "./data/milvus.db"  # milvus local
# uri = "http://localhost:19530"  # milvus remote
# token = "root:Milvus"  # milvus remote auth
# host = "localhost"  # chroma remote
# port = 8000  # chroma remote port

# ============================================================================
# Learning configuration
# ============================================================================
[learning]
verify_threshold = 0.7
reject_threshold = 0.3
retrieval_k = 5

# ============================================================================
# Knowledge configuration (knowledge module overrides)
# ============================================================================
[knowledge.base_config.embedding]
# Base settings
# api_key = "sk-xxx"  # Sensitive data should use env var: DATAPILLAR_EMBEDDING_API_KEY (defaults to global embedding)
# provider: supports openai, glm
provider = "openai"
model = "text-embedding-3-small"
# base_url = "https://api.openai.com/v1"  # Optional, set for custom endpoint
dimension = 1536

[knowledge.base_config.vector_store]
type = "lance"  # lance | chroma | milvus
path = "./data/vectors"  # lance/chroma local
# uri = "./data/milvus.db"  # milvus local
# uri = "http://localhost:19530"  # milvus remote
# token = "root:Milvus"  # milvus remote auth
# host = "localhost"  # chroma remote
# port = 8000  # chroma remote port

[knowledge.chunk_config]
mode = "general"  # general | parent_child | qa
preprocess = ["normalize_newlines", "remove_control", "collapse_whitespace"]

[knowledge.chunk_config.general]
max_tokens = 800
overlap = 120
# delimiter = "\n\n"  # Optional fixed delimiter

[knowledge.retrieve_config]
method = "hybrid"  # semantic | hybrid
top_k = 8
# score_threshold = 0.2

[knowledge.retrieve_config.tuning]
pool_k = 32
rrf_k = 60

[knowledge.retrieve_config.quality]
dedupe = true
dedupe_threshold = 0.92
max_per_document = 2

[knowledge.retrieve_config.rerank]
mode = "off"  # off | model | weighted
provider = "sentence_transformers"
model = "cross-encoder/ms-marco-MiniLM-L-6-v2"
score_mode = "rank"  # rank | normalize | raw

[knowledge.retrieve_config.rerank.params]
# batch_size = 16
# device = "cuda"

[knowledge.retrieve_config.inject]
# mode=system: auto-retrieve and inject a system message before each agent run
# mode=tool: expose a knowledge_retrieve tool so the agent can fetch on demand (smaller context)
# Note: MapReduce reducers do not inject knowledge; they only summarize map results.
mode = "tool"  # system | tool
max_tokens = 1200
max_chunks = 6
format = "markdown"  # markdown | json
