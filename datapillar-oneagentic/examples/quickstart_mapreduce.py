"""
Datapillar OneAgentic MapReduce æ¨¡å¼ç¤ºä¾‹

è¿è¡Œå‘½ä»¤ï¼š
    uv run python examples/quickstart_mapreduce.py
"""

from __future__ import annotations

import asyncio
import os

from pydantic import BaseModel

from datapillar_oneagentic import (
    AgentContext,
    Datapillar,
    DatapillarConfig,
    Process,
    agent,
    tool,
)


# ============================================================================
# LLM é…ç½®
# ============================================================================
LLM_PROVIDER = "glm"
LLM_API_KEY = os.environ.get("GLM_API_KEY")
LLM_BASE_URL = os.environ.get("GLM_BASE_URL")
LLM_MODEL = os.environ.get("GLM_MODEL")
LLM_ENABLE_THINKING = os.environ.get("GLM_ENABLE_THINKING", "false").lower() in {
    "1",
    "true",
    "yes",
}

if not LLM_API_KEY or not LLM_MODEL:
    raise RuntimeError("è¯·è®¾ç½® GLM_API_KEY å’Œ GLM_MODELï¼ˆå¯é€‰ GLM_BASE_URL/GLM_ENABLE_THINKINGï¼‰")


class TextOutput(BaseModel):
    text: str


@tool
def echo(text: str) -> str:
    """å›æ˜¾æ–‡æœ¬ã€‚

    Args:
        text: è¾“å…¥æ–‡æœ¬ã€‚

    Returns:
        å›æ˜¾ç»“æœã€‚
    """
    return f"echo:{text}"


@agent(
    id="worker_a",
    name="åˆ†æè€…",
    deliverable_schema=TextOutput,
    tools=[echo],
    description="æå–ä»»åŠ¡çš„å…³é”®ä¿¡æ¯ä¸è¦ç‚¹",
)
class WorkerAgentA:
    SYSTEM_PROMPT = """ä½ æ˜¯åˆ†æè€…ã€‚
ä½¿ç”¨ echo å·¥å…·æç‚¼ç”¨æˆ·è¾“å…¥çš„å…³é”®ä¿¡æ¯å¹¶ç»™å‡ºç»“è®ºã€‚

## è¾“å‡ºè¦æ±‚
åªèƒ½è¾“å‡º JSONï¼ˆå•ä¸ªå¯¹è±¡ï¼‰ï¼Œä¸å¾—è¾“å‡ºè§£é‡Šæˆ– Markdownï¼š
{"text": "ä½ çš„ç»“æœ"}
"""

    async def run(self, ctx: AgentContext) -> TextOutput:
        messages = ctx.build_messages(self.SYSTEM_PROMPT)
        messages = await ctx.invoke_tools(messages)
        return await ctx.get_structured_output(messages)


@agent(
    id="worker_b",
    name="æ€»ç»“è€…",
    deliverable_schema=TextOutput,
    tools=[echo],
    description="æ ¹æ®è¾“å…¥è¾“å‡ºæ€»ç»“æ€§ç»“è®º",
)
class WorkerAgentB:
    SYSTEM_PROMPT = """ä½ æ˜¯æ€»ç»“è€…ã€‚
ä½¿ç”¨ echo å·¥å…·è¾“å‡ºç®€çŸ­ç»“è®ºã€‚

## è¾“å‡ºè¦æ±‚
åªèƒ½è¾“å‡º JSONï¼ˆå•ä¸ªå¯¹è±¡ï¼‰ï¼Œä¸å¾—è¾“å‡ºè§£é‡Šæˆ– Markdownï¼š
{"text": "ä½ çš„ç»“æœ"}
"""

    async def run(self, ctx: AgentContext) -> TextOutput:
        messages = ctx.build_messages(self.SYSTEM_PROMPT)
        messages = await ctx.invoke_tools(messages)
        return await ctx.get_structured_output(messages)


@agent(
    id="reducer",
    name="æ±‡æ€»è€…",
    deliverable_schema=TextOutput,
    description="æ±‡æ€»å¤šè·¯ç»“æœå¹¶è¾“å‡ºæœ€ç»ˆç­”æ¡ˆ",
)
class ReducerAgent:
    SYSTEM_PROMPT = """ä½ æ˜¯æ±‡æ€»è€…ã€‚
æ±‡æ€»å¤šè·¯ç»“æœå¹¶ç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚

## è¾“å‡ºè¦æ±‚
åªèƒ½è¾“å‡º JSONï¼ˆå•ä¸ªå¯¹è±¡ï¼‰ï¼Œä¸å¾—è¾“å‡ºè§£é‡Šæˆ– Markdownï¼š
{"text": "ä½ çš„ç»“æœ"}
"""

    async def run(self, ctx: AgentContext) -> TextOutput:
        messages = ctx.build_messages(self.SYSTEM_PROMPT)
        return await ctx.get_structured_output(messages)


def _render_event(event: dict) -> None:
    event_type = event.get("event")
    if event_type == "agent.start":
        agent_info = event.get("agent", {})
        print(f"\nğŸ¤– [{agent_info.get('name')}] å¼€å§‹å·¥ä½œ...")
    elif event_type == "agent.thinking":
        message = event.get("message", {})
        thinking = message.get("content", "")
        if thinking:
            agent_info = event.get("agent", {})
            print(f"\nğŸ§  [{agent_info.get('id')}] æ€è€ƒä¸­...")
            print(f"   {thinking[:200]}..." if len(thinking) > 200 else f"   {thinking}")
    elif event_type == "tool.start":
        tool_info = event.get("tool", {})
        print(f"   ğŸ”§ è°ƒç”¨: {tool_info.get('name')}")
    elif event_type == "tool.end":
        tool_info = event.get("tool", {})
        result = str(tool_info.get("output", ""))
        if len(result) > 100:
            result = result[:100] + "..."
        print(f"   ğŸ“‹ ç»“æœ: {result}")
    elif event_type == "agent.end":
        print("   âœ… å®Œæˆ")
    elif event_type == "agent.interrupt":
        interrupt_payload = event.get("interrupt", {}).get("payload")
        print(f"\nâ“ éœ€è¦ç”¨æˆ·è¾“å…¥: {interrupt_payload}")
    elif event_type == "result":
        print(f"\n{'=' * 60}")
        print("ğŸ“¦ æœ€ç»ˆç»“æœ:")
        deliverables = event.get("result", {}).get("deliverable", {})
        for key, value in deliverables.items():
            print(f"\n[{key}]")
            if isinstance(value, dict):
                for k, v in value.items():
                    print(f"  {k}: {v}")
            else:
                print(f"  {value}")
    elif event_type == "error":
        error = event.get("error", {})
        print(f"\nâŒ é”™è¯¯: {error.get('detail') or error.get('message')}")


def create_mapreduce_team(config: DatapillarConfig) -> Datapillar:
    team = Datapillar(
        config=config,
        namespace="demo_mapreduce",
        name="MapReduce å›¢é˜Ÿç¤ºä¾‹",
        agents=[WorkerAgentA, WorkerAgentB, ReducerAgent],
        process=Process.MAPREDUCE,
        enable_share_context=True,
        verbose=True,
    )
    return team


async def main() -> None:
    llm_config = {
        "provider": LLM_PROVIDER,
        "api_key": LLM_API_KEY,
        "model": LLM_MODEL,
        "enable_thinking": LLM_ENABLE_THINKING,
        "timeout_seconds": 120,
        "retry": {"max_retries": 2},
    }
    if LLM_BASE_URL:
        llm_config["base_url"] = LLM_BASE_URL

    config = DatapillarConfig(llm=llm_config)
    team = create_mapreduce_team(config)

    print("=" * 60)
    print("ğŸ§© MapReduce æ¨¡å¼ç¤ºä¾‹å·²å°±ç»ª")
    print(f"   æ¨¡å‹: {LLM_MODEL}")
    print("   æˆå‘˜: åˆ†æè€… + æ€»ç»“è€… -> æ±‡æ€»è€…")
    print("=" * 60)

    query = "è¯·å°† Datapillar çš„æ ¸å¿ƒèƒ½åŠ›æ‹†æˆä¸¤éƒ¨åˆ†ï¼šè¦ç‚¹å’Œç»“è®ºã€‚"
    print(f"\nğŸ“ ç”¨æˆ·éœ€æ±‚: {query}\n")
    print("-" * 60)

    async for event in team.stream(query=query, session_id="s_demo_mapreduce"):
        _render_event(event)

    print("\n" + "=" * 60)
    print("âœ¨ æ¼”ç¤ºå®Œæˆ")


if __name__ == "__main__":
    asyncio.run(main())
