"""
ä¸Šä¸‹æ–‡å‹ç¼©å™¨

ç›´æ¥æ“ä½œ LangGraph çš„ messages åˆ—è¡¨ï¼Œå½“ token è¶…è¿‡é˜ˆå€¼æ—¶å‹ç¼©å†å²æ¶ˆæ¯ã€‚

å‹ç¼©æµç¨‹ï¼š
1. æ£€æŸ¥ token æ˜¯å¦è¶…è¿‡é˜ˆå€¼
2. ä¿ç•™æœ€è¿‘ N æ¡æ¶ˆæ¯ + ç”¨æˆ·æ¶ˆæ¯
3. å°†å…¶ä»–æ¶ˆæ¯å‹ç¼©ä¸ºæ‘˜è¦
4. è¿”å›å‹ç¼©åçš„ messages åˆ—è¡¨
"""

from __future__ import annotations

import logging
from typing import Any

from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
)

from datapillar_oneagentic.providers.token_counter import get_token_counter
from datapillar_oneagentic.context.compaction.compact_policy import CompactPolicy, CompactResult

logger = logging.getLogger(__name__)


class Compactor:
    """
    ä¸Šä¸‹æ–‡å‹ç¼©å™¨

    ç›´æ¥æ“ä½œ LangGraph çš„ messages åˆ—è¡¨ï¼ŒåŒ…æ‹¬ï¼š
    - æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
    - è°ƒç”¨ LLM ç”Ÿæˆæ‘˜è¦
    - è¿”å›å‹ç¼©åçš„ messages
    """

    def __init__(self, llm: Any, policy: CompactPolicy | None = None):
        """
        åˆå§‹åŒ–å‹ç¼©å™¨

        Args:
            llm: LLM å®ä¾‹
            policy: å‹ç¼©ç­–ç•¥
        """
        self.llm = llm
        self.policy = policy or CompactPolicy()
        self._token_counter = get_token_counter()

    def estimate_tokens(self, messages: list[BaseMessage]) -> int:
        """ä¼°ç®— messages çš„ token æ•°"""
        if not messages:
            return 0

        total = 0
        for msg in messages:
            content = msg.content if isinstance(msg.content, str) else str(msg.content)
            total += self._token_counter.count(content) + 10  # æ¶ˆæ¯å…ƒæ•°æ®å¼€é”€
        return total

    def needs_compact(self, messages: list[BaseMessage]) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦å‹ç¼©"""
        current_tokens = self.estimate_tokens(messages)
        trigger_tokens = self.policy.get_trigger_tokens()
        return current_tokens > trigger_tokens

    async def compact(
        self,
        messages: list[BaseMessage],
    ) -> tuple[list[BaseMessage], CompactResult]:
        """
        æ‰§è¡Œå‹ç¼©

        Args:
            messages: åŸå§‹æ¶ˆæ¯åˆ—è¡¨

        Returns:
            (å‹ç¼©åçš„ messages, CompactResult)
        """
        if not messages:
            return messages, CompactResult.no_action("æ²¡æœ‰æ¶ˆæ¯")

        tokens_before = self.estimate_tokens(messages)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
        if not self.needs_compact(messages):
            return messages, CompactResult.no_action(f"å½“å‰ {tokens_before} tokensï¼Œæ— éœ€å‹ç¼©")

        # åˆ†ç±»æ¶ˆæ¯ï¼šä¿ç•™ vs å‹ç¼©
        keep_messages, compress_messages = self._classify_messages(messages)

        if not compress_messages:
            return messages, CompactResult.no_action("æ²¡æœ‰å¯å‹ç¼©çš„æ¶ˆæ¯")

        # ç”Ÿæˆå‹ç¼©æ‘˜è¦
        try:
            summary = await self._generate_summary(compress_messages)
        except Exception as e:
            logger.error(f"å‹ç¼©å¤±è´¥: {e}", exc_info=True)
            return messages, CompactResult.failed(str(e))

        # æ„å»ºå‹ç¼©åçš„ messagesï¼šæ‘˜è¦ + ä¿ç•™çš„æ¶ˆæ¯
        compressed_messages = [
            SystemMessage(content=f"[å†å²æ‘˜è¦]\n{summary}"),
            *keep_messages,
        ]

        tokens_after = self.estimate_tokens(compressed_messages)

        logger.info(
            f"ğŸ“¦ å‹ç¼©å®Œæˆ: {len(compress_messages)} æ¡ â†’ æ‘˜è¦ï¼Œ"
            f"ä¿ç•™ {len(keep_messages)} æ¡ï¼Œ"
            f"èŠ‚çœ {tokens_before - tokens_after} tokens"
        )

        return compressed_messages, CompactResult(
            success=True,
            summary=summary,
            kept_count=len(keep_messages),
            removed_count=len(compress_messages),
            tokens_before=tokens_before,
            tokens_after=tokens_after,
            tokens_saved=tokens_before - tokens_after,
        )

    def _classify_messages(
        self,
        messages: list[BaseMessage],
    ) -> tuple[list[BaseMessage], list[BaseMessage]]:
        """
        åˆ†ç±»æ¶ˆæ¯

        è§„åˆ™ï¼š
        - æœ€è¿‘ min_keep_entries æ¡æ¶ˆæ¯å§‹ç»ˆä¿ç•™
        - HumanMessageï¼ˆç”¨æˆ·æ¶ˆæ¯ï¼‰å§‹ç»ˆä¿ç•™
        - å…¶ä»–æ¶ˆæ¯å‹ç¼©
        """
        min_keep = self.policy.get_min_keep_entries()

        if len(messages) <= min_keep:
            return messages.copy(), []

        # æœ€è¿‘çš„æ¶ˆæ¯å§‹ç»ˆä¿ç•™
        recent_messages = messages[-min_keep:]
        older_messages = messages[:-min_keep]

        keep_messages = []
        compress_messages = []

        for msg in older_messages:
            # ç”¨æˆ·æ¶ˆæ¯å§‹ç»ˆä¿ç•™
            if isinstance(msg, HumanMessage):
                keep_messages.append(msg)
            # SystemMessage ä¿ç•™ï¼ˆé€šå¸¸æ˜¯ system promptï¼‰
            elif isinstance(msg, SystemMessage):
                keep_messages.append(msg)
            else:
                compress_messages.append(msg)

        # åˆå¹¶ï¼šä¿ç•™çš„ + æœ€è¿‘çš„
        keep_messages.extend(recent_messages)

        return keep_messages, compress_messages

    async def _generate_summary(self, messages: list[BaseMessage]) -> str:
        """ç”Ÿæˆå‹ç¼©æ‘˜è¦"""
        # æ„å»ºå†å²æ–‡æœ¬
        history_lines = []
        for msg in messages:
            role = self._get_role_name(msg)
            content = msg.content if isinstance(msg.content, str) else str(msg.content)
            # æˆªæ–­è¿‡é•¿çš„å•æ¡æ¶ˆæ¯
            if len(content) > 500:
                content = content[:500] + "..."
            history_lines.append(f"[{role}] {content}")

        history_text = "\n".join(history_lines)

        # æ„å»ºå‹ç¼© prompt
        prompt = self.policy.compress_prompt_template.format(history=history_text)

        # è°ƒç”¨ LLM
        llm_messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªå¯¹è¯å†å²å‹ç¼©ä¸“å®¶ï¼Œè´Ÿè´£ç”Ÿæˆç»“æ„åŒ–çš„å¯¹è¯æ‘˜è¦ã€‚"),
            HumanMessage(content=prompt),
        ]

        response = await self.llm.ainvoke(llm_messages)
        summary = response.content.strip()

        # é™åˆ¶æ‘˜è¦é•¿åº¦
        max_tokens = self.policy.get_max_summary_tokens()
        while self._token_counter.count(summary) > max_tokens:
            paragraphs = summary.split("\n\n")
            if len(paragraphs) <= 1:
                summary = summary[: max_tokens * 2]
                break
            summary = "\n\n".join(paragraphs[:-1])

        return summary

    def _get_role_name(self, msg: BaseMessage) -> str:
        """è·å–æ¶ˆæ¯è§’è‰²å"""
        if isinstance(msg, HumanMessage):
            return "ç”¨æˆ·"
        elif isinstance(msg, AIMessage):
            name = getattr(msg, "name", None)
            return name if name else "AI"
        elif isinstance(msg, ToolMessage):
            return f"å·¥å…·:{getattr(msg, 'name', 'unknown')}"
        elif isinstance(msg, SystemMessage):
            return "ç³»ç»Ÿ"
        return "æœªçŸ¥"


# === å…¨å±€å‹ç¼©å™¨å·¥å‚ ===

_compactor_cache: Compactor | None = None


def get_compactor(policy: CompactPolicy | None = None) -> Compactor:
    """
    è·å–å‹ç¼©å™¨å®ä¾‹ï¼ˆå¸¦ç¼“å­˜ï¼‰

    Args:
        policy: å‹ç¼©ç­–ç•¥ï¼ˆå¯é€‰ï¼‰

    Returns:
        Compactor å®ä¾‹
    """
    global _compactor_cache

    if _compactor_cache is None:
        from datapillar_oneagentic.providers.llm import call_llm

        llm = call_llm(temperature=0.0)
        _compactor_cache = Compactor(llm=llm, policy=policy or CompactPolicy())

    return _compactor_cache


def clear_compactor_cache() -> None:
    """æ¸…ç©ºå‹ç¼©å™¨ç¼“å­˜ï¼ˆä»…æµ‹è¯•ç”¨ï¼‰"""
    global _compactor_cache
    _compactor_cache = None
